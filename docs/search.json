[
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html",
    "href": "Assignments/Week 4/Assignment_week_4.html",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "",
    "text": "You will work with longitudinal data from a growth curve study. The dataset captures weight measurements of 568 Asian children, recorded during clinic visits on up to five occasions. Ages range from approximately 6 weeks to 27 months. Your task is to model the children’s growth trajectories over time.\nThis dataset was sourced from the data library of the centre for Multilevel Modelling, University of Bristol."
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#introduction",
    "href": "Assignments/Week 4/Assignment_week_4.html#introduction",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "",
    "text": "You will work with longitudinal data from a growth curve study. The dataset captures weight measurements of 568 Asian children, recorded during clinic visits on up to five occasions. Ages range from approximately 6 weeks to 27 months. Your task is to model the children’s growth trajectories over time.\nThis dataset was sourced from the data library of the centre for Multilevel Modelling, University of Bristol."
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#dataset-description",
    "href": "Assignments/Week 4/Assignment_week_4.html#dataset-description",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "Dataset description",
    "text": "Dataset description\nThe dataset consists of the following variables:\n\nChildID: Unique identifier for each child\nAge: Child’s age at the time of measurement (months)\nWeight: Child’s weight at the time of measurement (grams)\nGender: Child’s gender (Boy or Girl)"
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#objectives",
    "href": "Assignments/Week 4/Assignment_week_4.html#objectives",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "Objectives",
    "text": "Objectives\nYour objectives are to:\n\nDevelop an appropriate model to describe the growth curve of children’s weight over time, using the “time structure first” approach.\nUse the model to answer the following research question: How does gender influence the growth trajectory of children’s weight?"
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#steps-to-complete-the-analysis",
    "href": "Assignments/Week 4/Assignment_week_4.html#steps-to-complete-the-analysis",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "Steps to complete the analysis",
    "text": "Steps to complete the analysis\n\nStep 1: Load and prepare the data\nDownload the dataset (see link in the Downloads section below) and read it into R. Ensure that the Gender variable is correctly encoded as a factor. Center the Age variable to aid interpretation.\n\n\n\n\n\n\nTipHint\n\n\n\nSince the dataset is in CSV format (not SPSS), we use read.csv() instead of read_sav() from the haven package. Character variables can be converted to factors using the same approach as in previous assignments:\n\n# Load the data\nasian_data &lt;- read.csv(\"downloads/asian_data.csv\")\n\n# Convert all character variables into factors\nasian_data &lt;- asian_data |&gt;\n  mutate(across(where(is.character), as.factor))\n\n\n\n\n\nStep 2: Exploratory data analysis\n\nCreate a spaghetti plot showing individual trajectories and the overall mean trajectory.\nExamine whether the mean trajectory appears linear or whether a quadratic (or higher-order) trend might be needed.\nCreate plots of mean trajectories by gender to visualize potential differences.\n\n\n\nStep 3: Model the time structure\nFollowing the “time structure first” approach from the lab:\n\nFit a flexible model for time: Start with a model that includes plausible polynomial terms for time (linear, quadratic, and possibly cubic given the age range) as fixed effects, along with random intercept and random slope.\nTest the random slope: Use a likelihood ratio test (refit = FALSE) to determine whether the random slope for time is necessary.\nCalculate the ICC: Based on the random intercept model, calculate and interpret the ICC.\nTest the fixed effects for time: Use likelihood ratio tests (refit = TRUE) to determine which polynomial terms are needed. Keep the random effects structure fixed during this step.\nCheck residual diagnostics: Create a residuals vs. fitted plot to verify that the time trend is adequately captured.\n\n\n\n\n\n\n\nTipHint: Using the bobyqa optimizer\n\n\n\nWhen fitting models with random slopes, you may encounter convergence warnings. To avoid these, use the bobyqa optimizer by adding control = lmerControl(optimizer = \"bobyqa\") to your lmer() calls:\n\nmodel &lt;- lmer(Weight ~ age_c + (age_c | ChildID), \n              data = asian_data,\n              control = lmerControl(optimizer = \"bobyqa\"))\n\n\n\n\n\nStep 4: Add the gender covariate\nOnce the time structure is established:\n\nFit a model that includes gender and its interaction with the time terms.\nTest whether gender affects the growth trajectory (main effect and/or interaction with time).\nSimplify the model if interaction terms are not significant.\n\n\n\nStep 5: Interpret the results\nInterpret the findings from your final model, focusing on:\n\nHow children’s weight changes over time on average\nWhether boys and girls differ in their baseline weight and/or growth rate\nThe variance components: how much do children vary in their baseline weight and growth rates?"
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#requirements-for-the-report",
    "href": "Assignments/Week 4/Assignment_week_4.html#requirements-for-the-report",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "Requirements for the report",
    "text": "Requirements for the report\nSubmit both the Quarto source file (.qmd) and the rendered HTML file. The HTML file should be rendered with code visibility enabled (echo = TRUE)."
  },
  {
    "objectID": "Assignments/Week 4/Assignment_week_4.html#downloads",
    "href": "Assignments/Week 4/Assignment_week_4.html#downloads",
    "title": "Assignment Week 4: Longitudinal Data Analysis",
    "section": "Downloads",
    "text": "Downloads\nDownload dataset"
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html",
    "href": "Assignments/Week 2/Assignment_week_2.html",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "",
    "text": "In this assignment, you will analyze a dataset that evaluates the effectiveness of different exercise routines on improving functional mobility in patients recovering from knee replacement surgery. The study uses a replicated randomized block design to account for individual variability among patients.\nEach patient participated in all three exercise routines, and each routine was performed three times. Functional mobility was assessed after each session, with scores ranging from 40 to 80 (higher scores indicate better mobility). The dataset contains 54 observations (6 patients x 3 routines x 3 replications) and is provided in the downloads section below."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#introduction",
    "href": "Assignments/Week 2/Assignment_week_2.html#introduction",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "",
    "text": "In this assignment, you will analyze a dataset that evaluates the effectiveness of different exercise routines on improving functional mobility in patients recovering from knee replacement surgery. The study uses a replicated randomized block design to account for individual variability among patients.\nEach patient participated in all three exercise routines, and each routine was performed three times. Functional mobility was assessed after each session, with scores ranging from 40 to 80 (higher scores indicate better mobility). The dataset contains 54 observations (6 patients x 3 routines x 3 replications) and is provided in the downloads section below."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#dataset-description",
    "href": "Assignments/Week 2/Assignment_week_2.html#dataset-description",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe dataset consists of the following variables:\n\nPatient: A numeric variable representing individual patients (blocks).\nExerciseRoutine: A factor variable representing the type of exercise routine:\n\nA: Low-impact walking exercises.\nB: Resistance training exercises.\nC: Balance and flexibility exercises.\n\nFunctionalMobility: A numeric variable (40-80) representing the mobility score measured after each session."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#objective",
    "href": "Assignments/Week 2/Assignment_week_2.html#objective",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "Objective",
    "text": "Objective\nYour goal is to determine:\n\nWhether there are significant differences in functional mobility scores between the three exercise routines.\nHow much of the variability in mobility scores can be attributed to:\n\nDifferences between patients.\nDifferences between exercise routines.\nResidual variability (within patient-treatment combinations).\n\n\nTo complete this analysis, follow the detailed steps outlined below."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#steps-to-complete-the-analysis",
    "href": "Assignments/Week 2/Assignment_week_2.html#steps-to-complete-the-analysis",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "Steps to Complete the Analysis",
    "text": "Steps to Complete the Analysis\n\n1. Load the dataset\nWhen loading the dataset in R, ensure that the ExerciseRoutine variable is treated as a factor. One way to achieve this is by specifying the column type explicitly using the colClasses argument in the read.csv() function. For example:\n\n# Load the dataset and specify ExerciseRoutine as a factor\ndataset &lt;- read.csv(\"functional_mobility.csv\", colClasses = c(\"ExerciseRoutine\" = \"factor\"))\n\n\n\n2. Perform an Exploratory Data Analysis (EDA)\n\nVisualize the data using an interaction plot.\nCalculate descriptive statistics for each exercise routine and patient.\n\n\n\n3. Fit a Mixed-Effects Model\n\nFit a mixed-effects model with:\n\nFixed Effects: ExerciseRoutine (to estimate differences between exercise routines).\nRandom Effects:\n\nPatient (to account for variability between individuals).\nPatient × ExerciseRoutine (to account for potential variability in how patients respond to different routines).\n\n\nSummarize the model:\n\nReport the fixed effects and variance components.\n\n\n\n\n4. Test for Overall Differences\n\nUse the selected model to test for overall differences in functional mobility scores across exercise routines.\nIf significant overall differences are found, perform pairwise comparisons to identify which routines differ.\n\n\n\n5. Assess variance components\n\nCalculate ICCs to evaluate the variance components\n\n\n\n6. Evaluate Model Assumptions\n\nAssess the model’s assumptions (e.g., normality of residuals, homoscedasticity) using diagnostic plots."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#requirements-for-the-report",
    "href": "Assignments/Week 2/Assignment_week_2.html#requirements-for-the-report",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "Requirements for the Report",
    "text": "Requirements for the Report\nSubmit both the Quarto source file (.qmd) and the rendered HTML file. The quarto source file should include all R code and annotations, and the html file should be rendered with code visibility enabled (echo = TRUE)."
  },
  {
    "objectID": "Assignments/Week 2/Assignment_week_2.html#downloads",
    "href": "Assignments/Week 2/Assignment_week_2.html#downloads",
    "title": "Assignment Week 2: Effectiveness of Exercise Routines on Functional Mobility",
    "section": "Downloads",
    "text": "Downloads\nDownload dataset"
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html",
    "href": "Assignments/Final/Final_assignment.html",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "",
    "text": "For this final assignment, you will write a short research report analyzing recovery trajectories in cardiac surgery patients."
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#assignment-overview",
    "href": "Assignments/Final/Final_assignment.html#assignment-overview",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "",
    "text": "For this final assignment, you will write a short research report analyzing recovery trajectories in cardiac surgery patients."
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#study-background",
    "href": "Assignments/Final/Final_assignment.html#study-background",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Study Background",
    "text": "Study Background\n\nClinical Context\nCoronary artery bypass grafting (CABG) is one of the most common cardiac surgeries, performed to restore blood flow to the heart in patients with severe coronary artery disease. While the surgery itself is generally successful, recovery is a gradual process that varies considerably between patients. Understanding the factors that influence recovery trajectories is crucial for optimizing patient care and rehabilitation strategies.\n\n\nThe RECOVER Study\nThe data for this assignment come from the RECOVER study (Recovery and Exercise after Cardiac Operation: Variability and Effectiveness Research), a randomized controlled trial that enrolled 500 patients undergoing CABG surgery (with or without valve repair). Patients were followed for 6 months post-surgery.\nStudy objectives:\n\nTo characterize recovery trajectories in quality of life and physical function after cardiac surgery\nTo identify patient characteristics associated with different recovery patterns\nTo evaluate the effectiveness of an enhanced cardiac rehabilitation program compared to standard care\n\nStudy design:\n\nPatients were assessed at 5 time points: pre-surgery (baseline), 2 weeks, 6 weeks, 3 months, and 6 months post-surgery\nPatients were randomized to receive either Standard rehabilitation (usual care with basic exercise guidance) or Enhanced rehabilitation (structured exercise program with additional psychological support and nutritional counseling)\nKey outcomes included health-related quality of life and functional capacity (6-minute walk test)\n\n\n\nOutcome Measures\nQuality of Life was assessed using a validated cardiac-specific quality of life questionnaire, yielding a score from 0 (worst) to 100 (best). This measure captures physical, emotional, and social aspects of health-related quality of life relevant to cardiac patients.\nWalk Distance was measured using the 6-minute walk test, a standardized assessment of functional capacity. Patients walk as far as possible in 6 minutes on a flat surface. The distance (in meters) reflects overall cardiopulmonary fitness and is a strong predictor of long-term outcomes in cardiac patients."
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#your-assignment",
    "href": "Assignments/Final/Final_assignment.html#your-assignment",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Your Assignment",
    "text": "Your Assignment\nUse the lookup table below to determine your subpopulation based on the last two digits of your student or staff number. You will analyze both outcomes (Quality of Life and Walk Distance) within your assigned subpopulation.\n\nSubpopulation Lookup Table\n\n\n\nLast 2 digits\nSubpopulation\n\n\n\n\n00-24\nFemale patients (sex == \"Female\")\n\n\n25-49\nMale patients (sex == \"Male\")\n\n\n50-74\nPatients with diabetes (diabetes == \"Yes\")\n\n\n75-99\nPatients without diabetes (diabetes == \"No\")\n\n\n\nFor example, if your student/staff number is 1234567, your last two digits are 67, so you analyze patients with diabetes.\n\n\nYour Research Questions\nAll students address the same two research questions within their assigned subpopulation:\n\nQuality of Life: Does the rehabilitation program (Standard vs Enhanced) affect the recovery trajectory of quality of life in [your subpopulation]?\nWalk Distance: Does the rehabilitation program (Standard vs Enhanced) affect the recovery trajectory of walk distance in [your subpopulation]?\n\n\n\nGetting Started\nDownload the template file and the dataset below. Place both files in the same folder, then open the template in RStudio.\n\n\n\n\n\n\nImportant\n\n\n\nIn the template, replace 123456 with your actual student/staff number in the setup code chunk, and update the author field in the YAML header with your name and student ID.\n\n\nAfter running the setup code chunk, your personalized dataset will be stored in my_data. Use this dataset for all subsequent analyses."
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#dataset-description",
    "href": "Assignments/Final/Final_assignment.html#dataset-description",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe dataset contains the following variables:\n\n\n\n\n\n\n\nVariable\nDescription\n\n\n\n\npatient_id\nUnique patient identifier\n\n\ntime_weeks\nTime in weeks (0, 2, 6, 12, 24)\n\n\nsex\nPatient sex (Male/Female)\n\n\ndiabetes\nDiabetes status (Yes/No)\n\n\nrehab_program\nRehabilitation program (Standard/Enhanced)\n\n\nquality_of_life\nQuality of Life score (0-100, higher = better)\n\n\nwalk_distance\n6-minute walk test distance in meters (higher = better)"
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#required-report-structure",
    "href": "Assignments/Final/Final_assignment.html#required-report-structure",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Required Report Structure",
    "text": "Required Report Structure\nYour report must follow the structure outlined below.\n\n\n1. Introduction\nBrief introduction to the study and your research questions.\n\nDescribe the clinical context (cardiac surgery recovery)\nIntroduce your assigned subpopulation and justify its clinical relevance\nState your two research questions (one for each outcome)\n\n\n\n\n2. Exploratory Data Analysis\n\nDistribution of both outcome variables in your subpopulation\nIndividual recovery trajectories (spaghetti plots) for both outcomes\nPatterns by rehabilitation program for each outcome\n\n\n\n\n3. Unconditional Growth Models\nFit unconditional growth models for both outcomes:\n\nReport fixed effects (intercept and slope)\nReport random effects (variance components)\nCalculate and interpret the ICC for each outcome\nCheck model assumptions\n\n\n\n\n4. Conditional Growth Models\nBuild models addressing your research questions for both outcomes:\n\nAdd rehabilitation program and its interaction with time\nTest whether the rehab effect is significant\nPresent your final models for quality of life and walk distance\n\n\n\n\n5. Discussion and Conclusions\n\nSummary of key findings\nClinical interpretation\nLimitations"
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#submission-requirements",
    "href": "Assignments/Final/Final_assignment.html#submission-requirements",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Submission Requirements",
    "text": "Submission Requirements\nSubmit both:\n\nQuarto source file (.qmd)\nRendered HTML file (.html)"
  },
  {
    "objectID": "Assignments/Final/Final_assignment.html#downloads",
    "href": "Assignments/Final/Final_assignment.html#downloads",
    "title": "Final Assignment: Longitudinal Analysis of Cardiac Recovery",
    "section": "Downloads",
    "text": "Downloads\n\nReport template\nDataset"
  },
  {
    "objectID": "BLR_lab_replicated_randomized_block.html",
    "href": "BLR_lab_replicated_randomized_block.html",
    "title": "Beyond MLR Lab 4: Replicated Randomized Block Designs",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages ggplot2, dplyr, emmeans, lmerTest, and broom.mixed. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(dplyr, ggplot2, emmeans, lmerTest, broom.mixed)\nIn last week’s lab, we considered a traditional randomized block design without replication, where each treatment was applied once within each block (laboratory). This design allowed us to control for variability between laboratories and efficiently estimate the effect of treatment. However, a key limitation of this approach is that it does not allow us to assess the extent to which the effect of treatment varies across blocks, a concept known as interaction.\nAn interaction occurs when the effect of one factor (e.g., treatment) is not consistent across levels of another factor (e.g., laboratory). In other words, the effect of a treatment might vary depending on the laboratory conditions. Replicating the treatments within each block allows us to estimate these interaction effects and better understand the variability in treatment outcomes across different settings.\nIn this lab, we extend the randomized block design by adding replication within blocks, so we can estimate the variability within each block and assess the extent to which treatment effects vary across blocks."
  },
  {
    "objectID": "BLR_lab_replicated_randomized_block.html#exploratory-data-analysis",
    "href": "BLR_lab_replicated_randomized_block.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 4: Replicated Randomized Block Designs",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTo explore the data and investigate the presence of a potential interaction between treatments and laboratories, we create an interaction plot:\n\n# Create an interaction plot\nggplot(data_rep_block, aes(x = Treatment, y = WoundHealing, \n  group = Laboratory, color = Laboratory)) +\n    stat_summary(fun = mean, geom = \"point\", size = 3) +\n    stat_summary(fun = mean, geom = \"line\") +\n    labs(title = \"Interaction Plot\",\n         y = \"Mean Wound Healing Measure\",\n         x = \"Treatment\") +\n    theme_minimal()\n\nIn addition to the interaction plot, we compute descriptive statistics to summarize the wound healing measures within treatments and within laboratories. This will help us assess variability both across treatments and across laboratory facilities.\n\n# Summarizing data by Treatment\nsummary_stats_treatment &lt;- data_rep_block |&gt;\n  group_by(Treatment) |&gt;\n  summarise(\n    n = n(),\n    Mean_WoundHealing = mean(WoundHealing),\n    SD_WoundHealing = sd(WoundHealing),\n    .groups = \"drop\"\n  )\n\n# Summarizing data by Laboratory\nsummary_stats_laboratory &lt;- data_rep_block |&gt;\n  group_by(Laboratory) |&gt;\n  summarise(\n    n = n(),\n    Mean_WoundHealing = mean(WoundHealing),\n    SD_WoundHealing = sd(WoundHealing),\n    .groups = \"drop\"\n  )\n\n# Displaying the produced summary tables\nsummary_stats_treatment\nsummary_stats_laboratory\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDoes the interaction plot indicate a potential interaction between treatment and laboratory?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDoes the plot suggest any differences in the overall effectiveness of the treatments (i.e., main effect of treatment) across all laboratories?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat does the plot suggest about variability between laboratories? Are some laboratories consistently higher or lower in wound healing across all treatments?"
  },
  {
    "objectID": "BLR_lab_replicated_randomized_block.html#a-mixed-effects-model-for-the-replicated-randomized-block-design",
    "href": "BLR_lab_replicated_randomized_block.html#a-mixed-effects-model-for-the-replicated-randomized-block-design",
    "title": "Beyond MLR Lab 4: Replicated Randomized Block Designs",
    "section": "A Mixed Effects Model for the Replicated Randomized Block Design",
    "text": "A Mixed Effects Model for the Replicated Randomized Block Design\nAs in the previous lab, laboratory is included as a random effect and treatment as a fixed effect in the model. The key new feature is the treatment-by-laboratory interaction term, which is also modeled as a random effect.\nTo understand why the interaction term must also be modeled as random, we need to consider the nature of random effects in this context. Since laboratory is treated as a random effect, we are assuming that the laboratories in the experiment represent a random sample from a larger population of possible laboratory conditions. This means that we are not just interested in the specific laboratories in the study, but in how the treatments would perform across any set of laboratories with varying conditions.\nWhen we include an interaction term between treatment and laboratory, we are asking whether the effect of each treatment depends on the laboratory environment. If we had modeled laboratory as a fixed effect (i.e., we were only interested in those specific laboratories), the interaction could also be treated as fixed. However, because laboratory is random, the interaction must also be treated as random to reflect the idea that the variability in treatment effects across laboratories applies not just to the specific laboratories in the study, but to any laboratory from the broader population.\n\nModel Specification\nThe mixed-effects model for the replicated randomized block design is specified as:\n\\[\nY_{ijk} = \\mu + \\tau_i + b_j + (tb)_{ij} + \\epsilon_{ijk}\n\\]\nwhere:\n\n\\(Y_{ijk}\\): Response (wound healing) for the \\(k\\)-tjh replicate of Treatment \\(i\\) in Laboratory \\(j\\).\n\\(\\mu\\): Overall mean response\n\\(\\tau_{i}\\): Fixed effect of Treatment \\(i\\).\n\\(b_{j}\\):Random effect of Laboratory \\(j\\), assumed to follow a normal distribution with mean zero and variance \\(\\sigma^2_{b}\\).\n\\((tb)_{ij}\\): Random interaction effect between treatment \\(i\\) and Laboratory \\(j\\), assumed to follow a normal distribution with mean zero and variance \\(\\sigma^2_{tb}\\).\n\\(\\epsilon_{ijk}\\): Residual error term, assumed to follow a normal distribution with mean zero and variance \\(\\sigma^2\\).\n\n\n\nModel Estimation\nSimilar as in the previous lab, we fit the mixed effects model using the lmer() function from the lmeTest package:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))  # Effects coding\n\nmodel_block &lt;- lmer(WoundHealing ~ Treatment + (1 | Laboratory) + \n  (1 | Treatment:Laboratory), data = data_rep_block)\nsummary(model_block)\n\nLet’s break down the model formula WoundHealing ~ Treatment + (1 | Laboratory) + (1 | Treatment:Laboratory):\n\nWoundHealing ~ Treatment: This specifies that WoundHealing is the outcome variable and that Treatment is included as a fixed effect to estimate differences between the three treatments.\n(1 | Laboratory): This specifies the random effect for laboratory, representing laboratory-specific deviations from the overall mean.\n(1 | Treatment:Laboratory): This specifies the random interaction term between Treatment and Laboratory.\n\n\n\nUnderstanding the model output\nWhen we run summary(model_block), the output provides two main components: fixed effects and variance components. Together, these summarize the estimated treatment differences and the variability captured by the random effects.\n\nFixed effects\n\nThe intercept represents the overall mean wound-healing measure across all treatments and laboratories (estimated as 56.47)\n\nThe coefficients for Treatment A (-1.04) and Treatment B (+1.03) indicate how each treatment’s mean differs from the overall mean\n\nWith effects coding, the coefficients must sum to zero, so the mean for Treatment C is slightly below the overall mean (≈ -0.01)\n\nThese coefficients represent the average treatment effects across all laboratories, after adjusting for differences in baseline wound-healing levels between laboratories.\n\n\nVariance components\nThe model includes two random terms that capture distinct sources of variability:\n\nThe laboratory-level random intercept (variance = 1.446) represents systematic differences in average wound healing between laboratories\n\nThe treatment-by-laboratory interaction (variance = 0.475) captures how treatment effects vary across laboratories\n\nThe residual variance (0.909) represents unexplained variability within each treatment–laboratory combination\n\n\n\n\nAssessing the fixed effect of Wound healing treatment\nThe fixed effect of wound healing treatment can be assessed by examining the ANOVA table from the mixed-effects model. This shows whether the average wound-healing differs significantly across treatments, after accounting for variability between laboratories and treatment–laboratory interactions.\n\n# Extract the ANOVA table\nanova(model_block)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nInspect the ANOVA table and report whether the overall treatment effect is statistically significant.\n\n\n\n\n\n\n\n\nImportantExercise\n\n\n\nUse the emmeans package to perform pairwise comparisons between treatments and explore which pairs differ.\n\n\n\n\nAssessing the variance components\nThe variance components can be assessed using the intra-class correlation coefficient (ICC), which expresses how much of the total unexplained variance arises from each random effect:\n\n# Extract variance components from the fitted model and\n# square the standard deviations to obtain variances\nvc &lt;- tidy(model_block, effects = \"ran_pars\") |&gt;\n  mutate(variance = estimate^2)\nvc\n\n# Compute ICCs from the variance components\nvar_total &lt;- sum(vc$variance)\nicc_lab &lt;- vc$variance[vc$group == \"Laboratory\"] / var_total\nicc_int &lt;- vc$variance[vc$group == \"Treatment:Laboratory\"] / var_total\nicc_lab\nicc_int\n\nExplanation: The tidy() function from the broom.mixed package takes complex model output and organizes it into a standardized tidy data frame, where each row corresponds to one model parameter (e.g., fixed effect, random-effect variance, residual variance). Here, we use it to extract the random-effect variance components from the fitted model object (argument effects = \"ran_pars\")\n\nThe laboratory ICC quantifies how strongly wound-healing outcomes are correlated within the same laboratory, reflecting baseline differences between labs\nThe interaction ICC quantifies the proportion of total variability attributable to differences in treatment effects across laboratories\n\nThe larger these ICCs, the greater the divergence between a mixed-effects model and an OLS analysis that ignores the grouping structure.\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow large is the ICC for the laboratory effect? What does this tell us about differences in baseline wound-healing across labs?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow large is the ICC for the treatment-by-laboratory interaction? Does this suggest that treatment effects vary meaningfully across laboratories, or are they relatively consistent?\n\n\n\n\nModel diagnostics\nThe process of performing model diagnostics for the replicated randomized block design is the same as for the traditional randomized block design. Therefore, we refer to the previous lab for detailed instructions on assessing model assumptions and performing diagnostic checks."
  },
  {
    "objectID": "BLR_lab_replicated_randomized_block.html#exploring-the-impact-of-omitting-the-random-interaction-term-from-the-mixed-effects-model",
    "href": "BLR_lab_replicated_randomized_block.html#exploring-the-impact-of-omitting-the-random-interaction-term-from-the-mixed-effects-model",
    "title": "Beyond MLR Lab 4: Replicated Randomized Block Designs",
    "section": "Exploring the impact of omitting the random interaction term from the mixed effects model",
    "text": "Exploring the impact of omitting the random interaction term from the mixed effects model\nTo explore how the inclusion of the random treatment-by-laboratory interaction impacts inference for the fixed effect of wound healing treatment, we now fit a simpler mixed-effects model that includes only a random intercept for laboratory.\n\n# Fit the model without random interaction term\nmodel_block_simple &lt;- lmer(WoundHealing ~ Treatment + (1 | Laboratory), data = data_rep_block)\nsummary(model_block_simple)\n\n\n# Print the ANOVA tables for the models without (top) \n# and with (bottom) random interaction terms\nanova(model_block_simple)\nanova(model_block)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nInspect the model summaries and ANOVA tables for both models. What do you obverve?\n\n\n\nExplanation\n\nThe simpler model assumes identical treatment effects across laboratories\n\nAll differences between labs are captured by a single random intercept\nThis ignores any lab-to-lab variation in how the treatments perform\nAs a result, treatment effects appear more precise (smaller SEs and lower p-values)\n\nThe full model allows treatment effects to vary across laboratories via a random Treatment×Laboratory term\n\nEncodes that laboratories may show different treatment contrasts\nTreats treatment-by-lab heterogeneity as additional uncertainty about the common effect\nIncreases the SE and typically the p-value for the overall treatment effect because lab-specific deviations reduce the information for estimating a single average effect\n\n\nIn short, the main treatment effect represents the average across all laboratories. When only a few labs are available, allowing for random treatment-by-lab heterogeneity makes that average more uncertain. In this example, the difference is minor because the interaction ICC is low (0.167), meaning that little variability arises from treatment-by-lab heterogeneity. When the interaction ICC is larger, treatment effects become less certain (reflected in larger standard errors and higher p-values) because more of the total variability is due to differences in treatment performance across laboratories."
  },
  {
    "objectID": "BLR_lab_longitudinal.html",
    "href": "BLR_lab_longitudinal.html",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages haven, ggplot2, dplyr, emmeans, lmerTest. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(haven, dplyr, ggplot2, emmeans, lmerTest)"
  },
  {
    "objectID": "BLR_lab_longitudinal.html#dataset-description",
    "href": "BLR_lab_longitudinal.html#dataset-description",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Dataset description",
    "text": "Dataset description\nThe dataset contains the following variables:\n\nOutcome variable:\n\nalcuse: Continuous measure of alcohol use based on various survey items.\n\nGrouping variable:\n\nid: Unique identifier for each adolescent in the study.\n\nCovariates:\n\ncoa: Dichotomous variable indicating parental alcoholism (1 = yes, 0 = no).\nsex: Dichotomous variable indicating sex.\npeer: Continuous measure of peer alcohol use, assessed at age 14.\nage: Numerical variable representing the age of the adolescent at each time point (14, 15, 16)."
  },
  {
    "objectID": "BLR_lab_longitudinal.html#research-question",
    "href": "BLR_lab_longitudinal.html#research-question",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Research question",
    "text": "Research question\nIn this lab, we are going to address the following research question:\n\nWhat factors predict adolescent alcohol use and its change over time? Specifically, do parental alcoholism, sex, and peer alcohol use influence baseline levels and trajectories of alcohol use from ages 14 to 16?"
  },
  {
    "objectID": "BLR_lab_longitudinal.html#data-import-cleaning-and-inspection",
    "href": "BLR_lab_longitudinal.html#data-import-cleaning-and-inspection",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Data import, cleaning, and inspection",
    "text": "Data import, cleaning, and inspection\n\n# Load the adolescent alcohol use data\n# Note: the call below assumes the dataset is placed in the 'data' folder\n# directly above the root folder of the project. Update the path as needed.\nalcohol_data &lt;- read_sav(\"data/alcoholpp.sav\")\n\n# Convert labelled variables (dbl+lbl) to factors using their value labels\nalcohol_data &lt;- alcohol_data |&gt; mutate(across(where(is.labelled), as_factor))\n\n# Use effects coding for the categorical variables\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\n# Inspect the structure of the dataset\nstr(alcohol_data)"
  },
  {
    "objectID": "BLR_lab_longitudinal.html#exploratory-data-analysis",
    "href": "BLR_lab_longitudinal.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Exploratory data analysis",
    "text": "Exploratory data analysis\n\nOverall pattern and individual variation\nWe begin by examining both the overall mean trajectory and individual trajectories simultaneously. This “spaghetti plot” shows all individual trajectories (thin gray lines) overlaid with the overall mean trajectory (thick blue line), providing an immediate sense of both the average pattern and the variability around it.\n\n# Compute the overall mean alcohol use by age\nmean_trajectory &lt;- alcohol_data |&gt;\n  group_by(age) |&gt;\n  summarise(mean_alcuse = mean(alcuse, na.rm = TRUE), .groups = \"drop\")\n\n# Create spaghetti plot: individual trajectories + mean trajectory\nggplot(alcohol_data, aes(x = age, y = alcuse)) +\n  geom_line(aes(group = id), alpha = 0.3, color = \"gray60\") +  # Individual trajectories\n  geom_line(data = mean_trajectory, aes(x = age, y = mean_alcuse), \n            color = \"steelblue\", linewidth = 1.5) +             # Mean trajectory\n  geom_point(data = mean_trajectory, aes(x = age, y = mean_alcuse), \n             color = \"steelblue\", size = 3) +                   # Mean points\n  labs(\n    title = \"Individual and Mean Alcohol Use Trajectories\",\n    subtitle = \"Gray lines = individual trajectories; Blue line = overall mean\",\n    x = \"Age\",\n    y = \"Alcohol Use\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the spaghetti plot:\n\nDoes the change in alcohol use over time appear to be linear, or would a quadratic trend be more appropriate?\nHow much variability is there in individual trajectories around the mean? Does this variability suggest the need for random effects in our models?\n\n\n\n\n\nExamining individual trajectories in detail\nWhile the spaghetti plot gives us a good overview of the overall variability, it can be difficult to distinguish specific individual patterns when all trajectories are overlaid. To get a better sense of the different types of individual trajectories, we’ll examine a random sample of 16 individuals using a facet plot.\n\nset.seed(123)  # Set seed for reproducibility\n\n# Sample 16 unique individuals\nsampled_ids &lt;- sample(1:82, 16)\n\n# Filter dataset for the sampled IDs and plot\nalcohol_data |&gt;\n  filter(id %in% sampled_ids) |&gt;\n  ggplot(aes(x = age, y = alcuse)) +\n  geom_line() +                               # Add trajectories (lines)\n  geom_point(size = 2, alpha = 0.7) +         # Add individual data points (dots)\n  facet_wrap(~ id) +                          # Create a subplot for each individual\n  labs(\n    title = \"Sample of Individual Alcohol Use Trajectories\",\n    x = \"Age\",\n    y = \"Alcohol Use\"\n  ) +\n  theme_minimal()\n\n\nObservations:\n\nMany adolescents report no alcohol use (alcuse = 0) across all time points.\nFor adolescents who do report alcohol use, the trajectories show diverse patterns: some increasing, some decreasing, and some remaining relatively stable over time.\nThe heterogeneity in both baseline levels and rates of change motivates the use of random intercepts and random slopes in our models.\n\n\n\n\n\n\n\nWarningDisclaimer\n\n\n\nThe dataset used in this lab is zero-inflated, with a large number of zero alcohol use values. While linear mixed-effects models are not necessarily the best approach for analyzing such data, we will use them in this lab to focus on the methodology. The results derived in this lab are intended for educational purposes only.\n\n\n\n\n\nMean trajectories by parental alcoholism\nNext, we will examine the mean trajectories of alcohol use by parental alcoholism (coa). This will provide an overview of how this variable relates to alcohol use over time.\n\n# Compute the mean alcohol use by age, and COA status\nmean_alcuse &lt;- alcohol_data |&gt;\n  group_by(age, coa) |&gt;\n  summarise(mean_alcuse = mean(alcuse, na.rm = TRUE), .groups = \"drop\")\n\n# Plot the mean trajectories by COA\nggplot(mean_alcuse, aes(x = age, y = mean_alcuse, color = coa)) +\n  geom_line() +\n  labs(\n    title = \"Mean Alcohol Use Trajectories by Parental Alcoholism\",\n    x = \"Age\",\n    y = \"Mean Alcohol Use\"\n  ) +\n  ylim(0, 3) +\n  theme_minimal()\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the plot, do you expect there to be a effect of parental alcoholism on alcohol use? Do you expect there to be an interaction effect between parental alcoholism and age? Explain your reasoning based on the trends shown in the plot.\n\n\n\n\n\n\n\n\nImportantCoding exercise\n\n\n\nCreate similar plots to explore mean trajectories by sex and by peer alcohol use.\nHint for peer alcohol use: Since peer is a continuous variable, you can categorize it into tertiles (low, medium, high) for visualization purposes using the following code:\nalcohol_data &lt;- alcohol_data |&gt;\n  mutate(peer_cat = cut(peer,\n                        breaks = quantile(peer, probs = c(0, 1/3, 2/3, 1), na.rm = TRUE),\n                        labels = c(\"Low\", \"Medium\", \"High\"),\n                        include.lowest = TRUE))\nThen compute mean trajectories by age and peer_cat, and create a plot similar to the one above."
  },
  {
    "objectID": "BLR_lab_longitudinal.html#modeling-the-time-structure",
    "href": "BLR_lab_longitudinal.html#modeling-the-time-structure",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Modeling the time structure",
    "text": "Modeling the time structure\nBefore adding covariates, we first establish the appropriate model for time. This “time structure first” approach ensures that we correctly capture how the outcome changes over time before investigating what factors might explain individual differences.\n\nStep 1: Fit a flexible model for time\nWe start with a model that includes:\n\nSaturated fixed effects for time: linear and quadratic terms (with 3 time points, this perfectly captures the mean trajectory)\nRandom intercept and random slope: allowing individuals to differ in both their baseline level and their rate of change\n\n\n# Center age at 15 (middle time point)\nalcohol_data$age_centered &lt;- alcohol_data$age - 15\n\n# Create quadratic term\nalcohol_data$age_centered_sq &lt;- alcohol_data$age_centered^2\n\n# Fit the full model with random intercept and random slope\ntime_model_full &lt;- lmer(alcuse ~ age_centered + age_centered_sq + (age_centered | id), \n                        data = alcohol_data,\n                        control = lmerControl(optimizer = \"bobyqa\"))\nsummary(time_model_full)\n\n\n\n\n\n\n\nTipUsing the bobyqa optimizer\n\n\n\nWe use control = lmerControl(optimizer = \"bobyqa\") to specify the BOBYQA (Bound Optimization BY Quadratic Approximation) optimizer. This derivative-free optimizer is often more robust than the default, especially for models with random slopes or when predictor variables are on different scales. It can help avoid convergence warnings that sometimes occur with the default optimizer.\n\n\n\n\n\n\n\n\nNoteUnderstanding the random effects syntax\n\n\n\nThe term (age_centered | id) specifies the random effects structure. This is shorthand for (1 + age_centered | id), which includes:\n\nRandom intercept (1): each individual has their own baseline level\nRandom slope (age_centered): each individual has their own rate of change over time\nCorrelation: the intercept and slope are allowed to be correlated (e.g., do individuals who start higher also change faster?)\n\nCompare this to (1 | id) used in previous labs, which only included a random intercept. The random slope is the key new element for longitudinal data, capturing the subject × time interaction discussed in the lecture.\nUncorrelated random effects: If you want to assume the intercept and slope are uncorrelated, use (1 | id) + (0 + age_centered | id) or equivalently (age_centered || id). This estimates separate variances but fixes the correlation to zero, reducing the number of parameters by one. This is sometimes used when the model with correlated random effects fails to converge, or when there is no theoretical reason to expect a correlation between baseline level and rate of change.\n\n\n\n\n\n\n\n\nNoteWhy not random quadratic slopes?\n\n\n\nWith only 3 observations per person, we cannot reliably estimate individual-specific quadratic trajectories (syntax: (age_centered + age_centered_sq | id)). The issue is one of identifiability: with 3 time points, we cannot distinguish between “person i has a different curvature” and “person i has random measurement error.”\nGeneral guideline:\n\n3 time points: Random intercept + random linear slope is feasible\n4+ time points: Could consider random quadratic slopes (though rarely needed)\nThe quadratic term remains as a fixed effect to model the average curvature\n\n\n\n\n\nStep 2: Test the random slope\nWe compare the full model (with random slope) against a model with only a random intercept using a likelihood ratio test. Since we are comparing random effects, we use refit = FALSE to retain REML estimation.\n\n# Fit a model with only random intercept\ntime_model_ri &lt;- lmer(alcuse ~ age_centered + age_centered_sq + (1 | id), \n                      data = alcohol_data,\n                      control = lmerControl(optimizer = \"bobyqa\"))\n\n# Compare models using likelihood ratio test\nanova(time_model_ri, time_model_full, refit = FALSE)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the likelihood ratio test, is the random slope for time necessary? What does this tell us about whether individuals differ in their rate of change in alcohol use?\n\n\n\n\nStep 3: Calculate the ICC\nRegardless of the random slope test result, we calculate the ICC from the random intercept model. This tells us what proportion of the variance in alcohol use (after accounting for time trends) is due to stable differences between individuals.\n\n# Extract variance components from the random intercept model\nvc &lt;- as.data.frame(VarCorr(time_model_ri))\nvar_intercept &lt;- vc$vcov[1]\nvar_residual &lt;- vc$vcov[2]\n\n# Calculate ICC\nicc &lt;- var_intercept / (var_intercept + var_residual)\ncat(\"ICC:\", round(icc, 3), \"\\n\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nInterpret the ICC. What does it tell you about the relative importance of between-person versus within-person variation in alcohol use?\n\n\n\n\nStep 4: Test the fixed effects for time\nNow we test whether the quadratic and linear time components are needed. We use likelihood ratio tests with refit = TRUE because we are comparing models with different fixed effects. We keep the random slope structure throughout.\n\n# Test quadratic term: compare model with vs without age_centered_sq\ntime_model_linear &lt;- lmer(alcuse ~ age_centered + (age_centered | id), data = alcohol_data,\n                          control = lmerControl(optimizer = \"bobyqa\"))\nanova(time_model_linear, time_model_full, refit = TRUE)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nIs the quadratic term significant? What does this tell you about the shape of the average trajectory?\n\n\n\n# Test linear term: compare model with vs without age_centered\ntime_model_null &lt;- lmer(alcuse ~ 1 + (age_centered | id), data = alcohol_data,\n                        control = lmerControl(optimizer = \"bobyqa\"))\nanova(time_model_null, time_model_linear, refit = TRUE)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nIs the linear term significant? What does this tell you about whether alcohol use changes over time on average?\n\n\n\n\nStep 5: Residual diagnostics\nAfter determining the time structure, we check whether the model adequately captures the time trend by examining the residuals.\n\n# Use the final time model (based on tests above, linear model with random slopes)\nfinal_time_model &lt;- time_model_linear\n\n# Create a data frame for plotting\ndiag_data &lt;- data.frame(\n  fitted = fitted(final_time_model),\n  residuals = resid(final_time_model)\n)\n\n# Residuals vs Fitted using ggplot2\nggplot(diag_data, aes(x = fitted, y = residuals)) +\n  geom_point(alpha = 0.5) +\n  geom_hline(yintercept = 0, color = \"red\", linewidth = 1) +\n  geom_smooth(method = \"loess\", se = TRUE, color = \"blue\", fill = \"lightblue\") +\n  labs(\n    title = \"Residuals vs Fitted\",\n    x = \"Fitted values\",\n    y = \"Conditional Residuals\"\n  ) +\n  theme_minimal()\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDoes the loess curve (blue line) lie close to zero across the range of fitted values?\n\n\n\n\n\n\n\n\nWarningInterpreting the diagnostic plot\n\n\n\nThe residual plot reveals patterns that are largely driven by the zero-inflated nature of the data. The clustering of points at low fitted values reflects the many adolescents with zero alcohol use. The loess curve dipping below zero at low fitted values and rising above zero at higher values, along with the increasing spread of residuals (heteroscedasticity), are typical of zero-inflated outcomes. While a linear mixed model provides a useful approximation for teaching purposes, more appropriate models for these data might include two-part (hurdle) models or zero-inflated models."
  },
  {
    "objectID": "BLR_lab_longitudinal.html#adding-covariates",
    "href": "BLR_lab_longitudinal.html#adding-covariates",
    "title": "Beyond MLR Lab 6: Longitudinal data analysis",
    "section": "Adding covariates",
    "text": "Adding covariates\nNow that we have established the appropriate time structure, we can investigate which covariates predict alcohol use and whether they modify the effect of time (cross-level interactions).\n\nStep 1: Fit a full model with all covariates\nWe start with a model that includes all predictors and their interactions with time, combined with the random-effects structure and fixed time structure determined above (linear time with random intercept and random slope).\n\n# Center peer at its mean for interpretability\nalcohol_data$peer_centered &lt;- alcohol_data$peer - mean(alcohol_data$peer, na.rm = TRUE)\n\n# Fit the full model with all predictors and their interactions with time\n# Fixed effects: linear time only (quadratic was not significant in Step 4)\n# Random effects: intercept and slope for age_centered (based on the test in Step 2)\nfull_model &lt;- lmer(\n  alcuse ~ age_centered * (coa + sex + peer_centered) + (age_centered | id),\n  data = alcohol_data,\n  control = lmerControl(optimizer = \"bobyqa\")\n)\nsummary(full_model)\n\n\n\nStep 2: Simplify the fixed-effects structure\nWe use the step() function to perform backward elimination of non-significant fixed effects.\n\n# Use step() to simplify fixed effects\nstep_result &lt;- step(full_model, reduce.fixed = TRUE, reduce.random = FALSE)\nprint(step_result)\n\n# Extract the final model\nfinal_model &lt;- get_model(step_result)\n\nThe step() function performs backward elimination of fixed effects, adhering to important model building principles such as the marginality principle (lower-order terms are retained if higher-order terms involving them remain in the model) and uses F-tests with Satterthwaite approximation for hypothesis testing.\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the output from step(), which fixed effects terms were removed from the model? Does the final model include any interaction effects with time (cross-level interactions), or only main effects?\n\n\n\n\nStep 3: Final model and interpretation\nBased on the model building process, we arrive at our final model. We can now interpret the results in detail:\n\n# Display the final model summary\nsummary(final_model)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the final model, what can you conclude about the factors that predict adolescent alcohol use and its change over time? Consider:\n\nWhich predictors have significant effects on alcohol use?\nAre there differences between groups (e.g., children of alcoholic parents vs. not, sex differences)?\nDo any predictors interact with time (age), suggesting different trajectories for different groups?"
  },
  {
    "objectID": "BLR_lab_FE_oneway.html",
    "href": "BLR_lab_FE_oneway.html",
    "title": "Beyond MLR Lab 1: One-way ANOVA",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages ggplot2, dplyr, and emmeans. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(dplyr, ggplot2, emmeans, lmerTest)"
  },
  {
    "objectID": "BLR_lab_FE_oneway.html#exploratory-data-analysis",
    "href": "BLR_lab_FE_oneway.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 1: One-way ANOVA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTo understand the distribution of blood pressure reduction across treatment groups, we start by creating a boxplot:\n\n# Visualizing the results\nggplot(data_oneway, aes(x = Treatment, y = BP_change, fill = Treatment)) +\n  geom_boxplot() +\n  labs(title = NULL,\n       x = \"Treatment\", y = \"Change in SBP (mm Hg)\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\nWe also calculate some summary statistics:\n\n# Generating summary statistics\nsummary_stats &lt;- data_oneway |&gt;\n  group_by(Treatment) |&gt;\n  summarise(\n    Mean_BP_change = mean(BP_change),\n    SD_BP_change = sd(BP_change)\n  )\nsummary_stats\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow do the mean changes in SBP compare among the treatment groups?"
  },
  {
    "objectID": "BLR_lab_FE_oneway.html#performing-a-one-way-anova",
    "href": "BLR_lab_FE_oneway.html#performing-a-one-way-anova",
    "title": "Beyond MLR Lab 1: One-way ANOVA",
    "section": "Performing a One-Way ANOVA",
    "text": "Performing a One-Way ANOVA\n\nModel Specification\nUsing effects coding, the one-way Analysis of Variance (ANOVA) model can be specified as:\n\\[\nY_{ij} = \\mu + \\tau_i + \\epsilon_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\): The change in SBP for the \\(j\\)-th patient in the \\(i\\)-th treatment group.\n\\(\\mu\\): The mean change in SBP across all treatment groups.\n\\(\\tau_i\\): The effect of the \\(i\\)-th treatment (deviation from the overall mean).\n\\(\\epsilon_{ij}\\): The residual error term, assumed to be independent and normally distributed with mean zero and constant variance.\n\n\n\n\n\n\n\nNoteCoding schemes for categorical variables\n\n\n\nIn regression models, categorical variables are represented using coding schemes that allow their inclusion as explanatory variables. While dummy coding is widely used in multiple linear regression models, effects coding is more common in ANOVA models as it enables interpretation of group differences relative to the grand mean rather than a single reference category. This interpretative difference makes effects coding particularly useful for examining group-level effects in experimental designs.\nTo change the coding scheme globally to effects coding:\n\n# Set effects coding as the default coding scheme for unordered (nominal) factors\n# Keep the default polynomial coding for ordered (ordinal) factors\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\nTo reset the coding scheme to the default dummy coding:\n\n# Reset the coding scheme for unordered factors to dummy coding\noptions(contrasts = c(\"contr.treatment\", \"contr.poly\"))\n\n\n\n\n\nModel Estimation\nWe can fit the one-way ANOVA model using the lm() function:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\")) # Effects coding\ncontrasts(data_oneway$Treatment) # Print contrast matrix for Treatment\n\n# Fit the one-way ANOVA model\nmodel_oneway &lt;- lm(BP_change ~ Treatment, data = data_oneway)\nsummary(model_oneway)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow do the estimated coefficients relate to the group means?\n\n\n\n\nANOVA Table\nTo test whether the effect of treatment is statistically significant, we use the anova() function to obtain the ANOVA table:\n\n# ANOVA table\nanova_results &lt;- anova(model_oneway)\nanova_results\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat does the F-test tell us about the treatment effect?\n\n\n\n\nEstimated Marginal Means\nEstimated marginal means, also known as least-squares means, are model-based means that represent the predicted (or expected) response at each level of a factor, averaged over the levels of other variables in the model.\nIn the context of a one-way ANOVA, where there is only one factor (e.g., treatment group), the estimated marginal means are equal to the observed group means. In more complex models with multiple factors, estimated marginal means provide predicted group means that are averaged over the levels of these additional factors. For example, in a model with both treatment and age as factors, the estimated marginal means for the treatment groups show the treatment means averaged over age, illustrating what these group means are expected to be at specific values of age (or averaged over a grid of age values).\nWe can calculate the estimated marginal means using the emmeans() function from the emmeans package:\n\n# Obtain estimated marginal means\nemms &lt;- emmeans(model_oneway, ~ Treatment)\nemms  # displays EMMs with standard errors and 95% confidence intervals\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow do the estimated marginal means compare to the observed group means calculated during the exploratory data analysis?\n\n\n\n\nPairwise comparisons\nSince we found a significant treatment effect, we will conduct pairwise comparisons of the estimated marginal means to identify which specific treatment groups differ significantly from one another. We perform these pairwise comparisons using the pairs() function from the emmeans package. To account for the increased risk of Type I error due to multiple comparisons, we apply the Bonferroni correction to adjust the p-values, ensuring that the overall significance level remains controlled.\n\n# Performing post-hoc analysis with emmeans\npairs(emms, adjust=\"Bonferroni\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the results of the pairwise comparisons, which treatment groups differ significantly?\n\n\n\n\nModel Diagnostics\nTo assess the adequacy of the fitted model, we create two diagnostic plots:\n\nResiduals vs Fitted Plot: Used to assess homoscedasticity (constant variance) for the errors. A random scatter of residuals around zero indicates equal variance.\nNormal Q-Q Plot: Used to assess normality of the errors. Residuals following the reference line suggest normally distributed errors.\n\n\n# Residuals vs Fitted\nplot(model_oneway, which = 1, main = \"Residuals vs Fitted\")\n\n# Normal Q-Q\nplot(model_oneway, which = 2, main = \"Normal Q-Q\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDo the diagnostic plots suggest any violations of the model assumptions?\n\n\n\n\nReporting\nA one-way ANOVA showed a statistically significant effect of treatment on the change in systolic blood pressure, F(2, 117) = 43.86, p &lt; 0.001. Estimated marginal means (95% CI) were -20.1 (-22.9, -17.2) mm Hg for Drug A, -4.9 (-7.8, -2.1) mm Hg for Drug B, and -1.6 (-4.4, 1.3) mm Hg for the Control group. Pairwise comparisons (Bonferroni-adjusted) indicated statistically significant differences between Drug A and both Drug B (p &lt; 0.001) and the Control group (p &lt; 0.001), but not between Drug B and the Control group (p = 0.291)."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "R labs for the Beyond MLR course",
    "section": "",
    "text": "This website hosts the R labs for the beyond MLR course. You can navigate between the different labs using the menu above."
  },
  {
    "objectID": "BLR_lab_MLA.html",
    "href": "BLR_lab_MLA.html",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages ggplot2, dplyr, emmeans, lmerTest. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(dplyr, ggplot2, emmeans, lmerTest)"
  },
  {
    "objectID": "BLR_lab_MLA.html#exploratory-data-analysis",
    "href": "BLR_lab_MLA.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nLet’s load the data and use the str() function to inspect the structure of the dataset.\n\n# Load the Chicago AirBnB data. Note that the call below assumes the csv file is placed in the 'data' folder directly above the root folder of the project. Update the string as needed if the file is located elsewhere.\nchicago_airbnb &lt;- read.csv(\"data/airbnb.csv\")\n\n# Convert all character variables into factors\nchicago_airbnb &lt;- chicago_airbnb |&gt;\n  mutate(across(where(is.character), as.factor))\n\n# Inspect the structure of the dataset\nstr(chicago_airbnb)\n\nThis initial inspection makes clear that the Airbnb dataset has a hierarchical structure: 1,561 individual listings are nested within 43 neighborhoods. While the dataset also includes a district variable that groups neighborhoods into 9 broader districts, for educational purposes we will treat this as a two-level hierarchical structure with listings (level 1) nested within neighborhoods (level 2), ignoring the higher-level clustering of neighborhoods into districts.\n\nGrouping structure\nTo get a better feeling for the grouping structure, we create a summary table showing the number of listings within each neighborhood and visualize it with a bar chart:\n\n# Create a summary table showing listings per neighborhood\nneighborhood_summary &lt;- chicago_airbnb |&gt;\n  group_by(neighborhood) |&gt;\n  summarise(num_listings = n(), .groups = \"drop\") |&gt;\n  arrange(desc(num_listings))\n\n# Create a bar chart showing listings per neighborhood (top 20)\nneighborhood_summary |&gt;\n  top_n(20, num_listings) |&gt;\n  ggplot(aes(x = reorder(neighborhood, num_listings), y = num_listings)) +\n  geom_bar(stat = \"identity\", fill = \"skyblue\", color = \"black\") +\n  coord_flip() +\n  labs(title = \"Number of Listings per Neighborhood (Top 20)\",\n       x = \"Neighborhood\",\n       y = \"Number of Listings\") +\n  theme_minimal()\n\nExplanation:\n\ngroup_by(neighborhood): groups the data by neighborhood so that we can count listings within each neighborhood.\nn(): counts the total number of listings within each neighborhood.\narrange(desc(num_listings)): sorts the table by the number of listings in descending order.\ntop_n(20, num_listings): selects the 20 neighborhoods with the most listings for visualization.\nreorder(neighborhood, num_listings): reorders the neighborhood factor levels based on the values of num_listings, arranging neighborhoods from lowest to highest number of listings for easier interpretation.\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhich neighborhoods have the most Airbnb listings? Is there substantial variation in the number of listings across neighborhoods?\n\n\n\n\nOutcome variable\nNext, we create a histogram to visualize the distribution of the outcome variable price:\n\n# Create a histogram of price\nggplot(chicago_airbnb, aes(x = price)) +\n  geom_histogram(bins = 30, fill = \"skyblue\", color = \"black\") +\n  labs(title = \"Distribution of Listing Prices\",\n       x = \"Nightly Price (USD)\",\n       y = \"Frequency\") +\n  theme_minimal()\n\nWhile normality of the outcome is not strictly required for a mixed-effects model, transforming a right-skewed variable like price can help stabilize variance and linearize relationships. We therefore create a new variable, log_price, to store the log-transformed prices:\n\n# Log-transform the price variable\nchicago_airbnb &lt;- chicago_airbnb |&gt;\n  mutate(log_price = log(price))\n\n\n\nVariance components\nWe also want to get a sense of the variability in the listing prices within and between neighborhoods. For this, we are going to randomly select 10 neighborhoods and create a scatter plot with the log-transformed price on the y-axis and the neighborhood on the x-axis:\n\n# Set seed for reproducibility\nset.seed(123)\n\n# Randomly select 10 unique neighborhoods\nrandom_neighborhoods &lt;- sample(unique(chicago_airbnb$neighborhood), size = 10)\n\n# Filter the data to include only the randomly selected neighborhoods\nrandom_neighborhood_data &lt;- chicago_airbnb |&gt;\n  filter(neighborhood %in% random_neighborhoods)\n\n# Calculate the average log_price for each neighborhood\nneighborhood_avg_price &lt;- random_neighborhood_data |&gt;\n  group_by(neighborhood) |&gt;\n  summarise(avg_log_price = mean(log_price, na.rm = TRUE), .groups = \"drop\")\n\n# Calculate the overall grand mean across all neighborhoods\ngrand_mean &lt;- mean(chicago_airbnb$log_price, na.rm = TRUE)\n\n# Create scatter plot with jittered observations\nggplot(random_neighborhood_data, aes(x = reorder(neighborhood, log_price, FUN = mean), y = log_price)) +\n  geom_hline(yintercept = grand_mean, linetype = \"solid\", color = \"black\", linewidth = 1) +\n  geom_jitter(alpha = 0.3, width = 0.2, size = 1, color = \"gray60\") +\n  geom_point(data = neighborhood_avg_price, aes(x = neighborhood, y = avg_log_price),\n             size = 4, shape = 18, color = \"red\") +\n  labs(title = \"Variation in listing prices: within and between neighborhoods\",\n       subtitle = \"Points = individual listings; Diamonds = neighborhood means; Solid line = grand mean (all data)\",\n       x = \"Neighborhood (ordered by mean log price)\",\n       y = \"Log-transformed price\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat does this plot suggest about the variability in listing prices within and between neighborhoods?\n\n\n\n\nCovariate effects\nBefore fitting models with covariates, it is useful to explore the relationships between potential predictors and the outcome variable. Let us examine how some subject-level characteristics relate to listing prices.\nFirst, we will look at the relationship between the number of reviews and log-transformed price:\n\n# Scatterplot with smooth line for reviews vs log_price\nggplot(chicago_airbnb, aes(x = reviews, y = log_price)) +\n  geom_point(color = \"gray60\") +\n  geom_smooth(method = \"lm\", se = FALSE, color = \"steelblue\", linewidth = 1.5) +\n  labs(title = \"Relationship between number of reviews and listing price\",\n       x = \"Number of reviews\",\n       y = \"Log-transformed price\") +\n  theme_minimal()\n\nNext, let us examine how listing prices vary across different room types:\n\n# Boxplot for room_type vs log_price\nggplot(chicago_airbnb, aes(x = room_type, y = log_price, fill = room_type)) +\n  geom_boxplot(alpha = 0.7) +\n  labs(title = \"Listing prices by room type\",\n       x = \"Room type\",\n       y = \"Log-transformed price\") +\n  theme_minimal() +\n  theme(legend.position = \"none\",\n        axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\nImportantCoding exercise\n\n\n\nCreate similar visualizations to explore the relationships between:\n\nOverall satisfaction rating and log-transformed price (scatterplot with smooth line)\nNumber of bedrooms and log-transformed price (boxplot)\n\nWhat patterns do you observe? Do higher satisfaction ratings and more bedrooms tend to be associated with higher or lower prices?"
  },
  {
    "objectID": "BLR_lab_MLA.html#random-intercept-model",
    "href": "BLR_lab_MLA.html#random-intercept-model",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "Random intercept model",
    "text": "Random intercept model\nTo model the variability in listing prices within and between neighborhoods, we start by fitting a random intercept model to account for the nesting of listings within neighborhoods. This model is specified as follows:\n\n# Fit the random intercept model\nrandom_intercept_model &lt;- lmer(log_price ~ 1 + (1 | neighborhood), data = chicago_airbnb)\nsummary(random_intercept_model)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the estimated variance components, what can you conclude about the relative contributions of within-neighborhood and between-neighborhood variability to the total variability in listing prices?"
  },
  {
    "objectID": "BLR_lab_MLA.html#extending-the-random-intercept-model-with-subject-level-variables",
    "href": "BLR_lab_MLA.html#extending-the-random-intercept-model-with-subject-level-variables",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "Extending the random intercept model with subject-level variables",
    "text": "Extending the random intercept model with subject-level variables\nAs a second step, we extend the random intercept model with subject-level variables. To facilitate the interpretation of the model coefficients, we start by setting the coding scheme for categorical variables to effects coding. We also center the numerical variables by subtracting the mean value from each observation. This step is important to reduce multicollinearity, improve numerical stability, and make the intercept more interpretable.\n\n# Use effects coding for the categorical variables\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))\n\n# Center the continuous subject-level variables\nchicago_airbnb &lt;- chicago_airbnb |&gt;\n  mutate(overall_satisfaction_c = scale(overall_satisfaction, scale = FALSE),\n         accommodates_c = scale(accommodates, scale = FALSE),\n         bedrooms_c = scale(bedrooms, scale = FALSE),\n         minstay_c = scale(minstay, scale = FALSE))\n\nNext, we fit the random intercept model with the subject-level variables included:\n\n# Fit the random intercept model with subject-level variables\nrandom_intercept_model_L1variables &lt;- lmer(log_price ~ overall_satisfaction_c + room_type + accommodates_c + bedrooms_c + minstay_c + (1 | neighborhood), data = chicago_airbnb)\nsummary(random_intercept_model_L1variables)\n\n# Display the coding scheme for the room_type variable\ncontrasts(chicago_airbnb$room_type)\n\n# Obtain the ANOVA table for the subject-level variables\nanova(random_intercept_model_L1variables)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow does centering the continuous variables affect the interpretation of the intercept?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow does the inclusion of individual-level variables affect the estimated variance components? More specifically, does the inclusion of individual-level variables affect the within-neighborhood variance, the between-neighborhood variance, or both? Can you explain why?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhich of the individual-level variables are significantly associated with listing prices? How do you interpret the coefficients for these variables?"
  },
  {
    "objectID": "BLR_lab_MLA.html#including-context-level-variables",
    "href": "BLR_lab_MLA.html#including-context-level-variables",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "Including context-level variables",
    "text": "Including context-level variables\nAs a third step, we extend the previously fitted model with context-level variables. We start by centering the context-level variables:\n\n# Center the context-level variables\nchicago_airbnb &lt;- chicago_airbnb |&gt;\n  mutate(WalkScore_c = scale(WalkScore, scale = FALSE),\n         TransitScore_c = scale(TransitScore, scale = FALSE),\n         BikeScore_c = scale(BikeScore, scale = FALSE))\n\nNext, we fit the random intercept model with both subject-level and context-level variables included:\n\n# Fit the random intercept model with subject-level and context-level variables\nrandom_intercept_model_L1L2variables &lt;- lmer(log_price ~ overall_satisfaction_c + room_type + accommodates_c + bedrooms_c + minstay_c + WalkScore_c + TransitScore_c + BikeScore_c + (1 | neighborhood), data = chicago_airbnb)\nsummary(random_intercept_model_L1L2variables)\n\n# Obtain the ANOVA table\nanova(random_intercept_model_L1L2variables)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow does the inclusion of context-level variables affect the estimated variance components? More specifically, does the inclusion of context-level variables affect the within-neighborhood variance, the between-neighborhood variance, or both? Can you explain why?\n\n\n\nModel selection using stepwise elimination\nNot all the context-level variables included in the model are significant predictors of listing prices. To obtain a more parsimonious model, we can use the step() function from the lmerTest package to perform a backward elimination of the fixed-effect terms.\nThe step() function implements backward elimination in two stages:\n\nFirst stage (random effects): By default, step() first eliminates random-effect terms using likelihood ratio tests (via the ranova() function)\nSecond stage (fixed effects): Then it eliminates fixed-effect terms by repeatedly calling drop1(), which uses F-tests\n\nBackward elimination process for fixed effects:\n\nStarting with the full covariate model (after any random effects elimination), drop1() computes F-tests for all marginal fixed-effect terms (i.e., terms that can be dropped while respecting the hierarchy of terms in the model)\nstep() identifies the term with the highest (least significant) p-value from the F-tests\nIf that p-value exceeds the significance threshold (default α = 0.05 for fixed effects), the term is removed\nThe process repeats with the new reduced model until no more terms can be removed\n\nThe F-tests used by drop1() are based on Satterthwaite’s approximation for the denominator degrees of freedom, which is appropriate for mixed models.\n\n\n\n\n\n\nNoteControlling what step() eliminates\n\n\n\nThe drop1() function is specifically designed for testing fixed effects. The step() function, by default, can eliminate both random effects (using likelihood ratio tests) and fixed effects (using F-tests). However, this behavior can be controlled using the reduce.random and reduce.fixed parameters:\n\nreduce.random = TRUE (default): allows elimination of random effects\nreduce.fixed = TRUE (default): allows elimination of fixed effects\n\nIn this lab, we focus on selecting fixed effects for our random intercept model, so we will set reduce.random = FALSE to prevent any changes to the random effect structure. Testing different random effect structures (such as whether to include random slopes) will be covered in more detail in the next lab on longitudinal data analysis.\n\n\n\n# Perform stepwise selection to identify the most important predictors\n# Set reduce.random = FALSE to only eliminate fixed effects (not random effects)\nstep(random_intercept_model_L1L2variables, reduce.random = FALSE)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhich individual-level and context-level variables are retained in the final model after the stepwise selection procedure?"
  },
  {
    "objectID": "BLR_lab_MLA.html#exploring-cross-level-interactions",
    "href": "BLR_lab_MLA.html#exploring-cross-level-interactions",
    "title": "Beyond MLR Lab 5: multi-level analysis of cross-sectional observational data",
    "section": "Exploring cross-level interactions",
    "text": "Exploring cross-level interactions\nFinally, we explore the possibility of cross-level interactions between individual-level and context-level variables.\nThe relationship between the overall satisfaction rating of an individual listing and its price may depend on neighborhood characteristics such as their walkability and access to public transit. For instance, higher satisfaction ratings might have a stronger effect on prices in less walkable or transit-accessible neighborhoods, where positive reviews could help compensate for the disadvantages of limited walkability or transit options. In contrast, in neighborhoods with high walkability or access to public transit ratings, these location-based amenities might already drive prices, reducing the added impact of satisfaction ratings.\nTo test whether the overall_satisfaction × WalkScore interaction is significant, we can fit a model with the interaction and use drop1() or anova() to test its contribution:\n\n# Fit model with WalkScore interaction\nmodel_interaction_walk &lt;- lmer(log_price ~ overall_satisfaction_c * WalkScore_c + room_type + accommodates_c + bedrooms_c + minstay_c + TransitScore_c + BikeScore_c + (1 | neighborhood), data = chicago_airbnb)\n\n# Test the interaction term\ndrop1(model_interaction_walk)\n\nThe drop1() output will show whether the interaction term overall_satisfaction_c:WalkScore_c significantly contributes to the model. Look for the p-value in the Pr(&gt;F) column for this interaction term.\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the F-test from drop1(), is the overall_satisfaction × WalkScore interaction statistically significant? What does this tell you about whether the effect of satisfaction on price depends on neighborhood walkability?\n\n\n\nInterpreting the interaction\nIf the interaction is significant, we can use the emmeans package to compute estimated marginal means (EMMs) at specific combinations of satisfaction ratings and walkability scores. These estimated marginal means represent the model’s predicted log prices at particular values of the predictors, holding all other variables constant at their reference levels. By calculating EMMs at representative values (such as low, average, and high levels), we can create a clear visualization of how the effect of satisfaction on price varies across different walkability levels.\n\n# Use representative values based on the data distribution\n# For interpretability, we'll use low (Q1), medium (mean = 0), and high (Q3) values\nsatisfaction_low &lt;- quantile(chicago_airbnb$overall_satisfaction_c, 0.25, na.rm = TRUE)\nsatisfaction_high &lt;- quantile(chicago_airbnb$overall_satisfaction_c, 0.75, na.rm = TRUE)\n\nwalkscore_low &lt;- quantile(chicago_airbnb$WalkScore_c, 0.25, na.rm = TRUE)\nwalkscore_high &lt;- quantile(chicago_airbnb$WalkScore_c, 0.75, na.rm = TRUE)\n\n# Create a 3x3 grid using low (Q1), average (0), and high (Q3) values\npredictions_grid &lt;- emmeans(model_interaction_walk,\n                            specs = ~ overall_satisfaction_c * WalkScore_c,\n                            at = list(overall_satisfaction_c = c(satisfaction_low, 0, satisfaction_high),\n                                     WalkScore_c = c(walkscore_low, 0, walkscore_high)),\n                            lmer.df = \"satterthwaite\")\n\n# Display the 3x3 grid of predictions\nprint(predictions_grid)\n\nExplanation of the emmeans() arguments:\n\nThe first argument is the fitted model object (model_interaction_walk)\nspecs = ~ overall_satisfaction_c * WalkScore_c specifies which variables to compute marginal means for, including their interaction\nat = list(...) specifies the exact values at which to evaluate the predictors. Here we use three levels for each variable: Q1 (low), 0 (average, since variables are centered), and Q3 (high)\nlmer.df = \"satterthwaite\" specifies the method for computing degrees of freedom and confidence intervals. We use Satterthwaite’s approximation to be consistent with the default method used by lmerTest for F-tests\nAll other predictors in the model (room type, accommodates, bedrooms, etc.) are automatically held at their reference levels or means\n\nThe output shows a 3 × 3 grid of estimated marginal means with their standard errors and confidence intervals, representing the estimated log price at each combination of satisfaction and walkability levels.\nNow we can visualize these estimated marginal means using an interaction plot:\n\n# Convert predictions to data frame for plotting\npred_df &lt;- as.data.frame(predictions_grid)\n\n# Add descriptive labels for the factor levels\npred_df$satisfaction_label &lt;- factor(pred_df$overall_satisfaction_c,\n                                     levels = c(satisfaction_low, 0, satisfaction_high),\n                                     labels = c(\"Low (Q1)\", \"Average (Mean)\", \"High (Q3)\"))\n\npred_df$walkscore_label &lt;- factor(pred_df$WalkScore_c,\n                                  levels = c(walkscore_low, 0, walkscore_high),\n                                  labels = c(\"Low (Q1)\", \"Average (Mean)\", \"High (Q3)\"))\n\n# Create interaction plot\nggplot(pred_df, aes(x = satisfaction_label, y = emmean,\n                    color = walkscore_label, group = walkscore_label)) +\n  geom_point(size = 3) +\n  geom_line(linewidth = 1) +\n  labs(title = \"Interaction between Satisfaction and Walkability\",\n       subtitle = \"Estimated log prices at different combinations of satisfaction and walkability\",\n       x = \"Overall Satisfaction Level\",\n       y = \"Estimated Log Price\",\n       color = \"Walkability Level\") +\n  theme_minimal() +\n  theme(legend.position = \"bottom\")\n\nThis visualization shows:\n\nThree lines representing neighborhoods with low, average, and high walkability\nSlopes indicating how satisfaction relates to price at each walkability level\nIf the lines are parallel, the interaction effect is weak or non-significant\nIf the lines have different slopes, it indicates that the satisfaction-price relationship varies by walkability\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the interaction plot, how does the relationship between satisfaction ratings and listing prices differ across neighborhoods with varying walkability levels? Do satisfaction ratings matter more (have a stronger effect) in low-walkability or high-walkability neighborhoods?\n\n\n\n\n\n\n\n\nImportantCoding exercise\n\n\n\nFollowing the example above, test whether the overall_satisfaction × TransitScore interaction is statistically significant.\n\nFit a model that includes the overall_satisfaction × TransitScore interaction\nUse drop1() to test whether the interaction term is significant\nInterpret the results: Is the interaction significant? What does this suggest about how satisfaction ratings relate to prices in neighborhoods with different levels of transit accessibility?\n\n\n\n\n\n\n\n\n\nNoteTesting strategy for mixed models\n\n\n\nIn this lab, we have used F-tests (via step() and drop1()) to test fixed effects in our mixed models. These tests:\n\nUse Satterthwaite’s approximation for degrees of freedom\nWork with REML estimation (the default for lmer(), which gives better variance component estimates)\nAre appropriate for testing whether fixed effect terms should be included in the model\n\nThis approach provides a consistent workflow for testing and selecting fixed effects. For testing random effects (such as whether to include random slopes in addition to random intercepts), we use likelihood ratio tests (LRT), which we will cover in the next lab on longitudinal data analysis.\n\n\n\n\nModel diagnostics\nThe process of performing model diagnostics for the random intercept models fitted in this lab is similar to the one described in the previous labs. Therefore, we refer to those labs for more detailed instructions on assessing model assumptions."
  },
  {
    "objectID": "BLR_lab_randomized_block.html",
    "href": "BLR_lab_randomized_block.html",
    "title": "Beyond MLR Lab 2: Randomized Block Designs",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages ggplot2, dplyr, emmeans, and lmerTest. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(dplyr, ggplot2, emmeans, lmerTest)"
  },
  {
    "objectID": "BLR_lab_randomized_block.html#exploratory-data-analysis",
    "href": "BLR_lab_randomized_block.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 2: Randomized Block Designs",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTo explore the data and investigate the possible presence of a laboratory effect, we start by creating a scatter plot with the wound healing variable on the x-axis and the laboratory facility variable on the y-axis:\n\nggplot(data_block, aes(x = WoundHealing, y = Laboratory, color = Treatment)) +\n  geom_point(size = 4) +\n  labs(title = \"Wound Healing by Treatment and Laboratory\",\n       x = \"Wound Healing Measure\",\n       y = \"Laboratory\") +\n  theme_minimal()\n\nIn addition to the scatter plot, we compute descriptive statistics to summarize the wound healing measures within treatments and within laboratories. This will help us assess variability both across treatments and across laboratory facilities.\n\n# Summarizing data by Treatment\nsummary_stats_treatment &lt;- data_block |&gt;\n  group_by(Treatment) |&gt;\n  summarise(\n    Mean_WoundHealing = mean(WoundHealing),\n    SD_WoundHealing = sd(WoundHealing)\n  )\n\n# Summarizing data by Laboratory\nsummary_stats_laboratory &lt;- data_block |&gt;\n  group_by(Laboratory) |&gt;\n  summarise(\n    Mean_WoundHealing = mean(WoundHealing),\n    SD_WoundHealing = sd(WoundHealing)\n  )\n\n# Displaying the produced summary tables\nsummary_stats_treatment\nsummary_stats_laboratory\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat patterns do you observe regarding the effects of treatment and laboratory based on the plot and summary statistics?"
  },
  {
    "objectID": "BLR_lab_randomized_block.html#a-mixed-effects-model-for-the-randomized-block-design",
    "href": "BLR_lab_randomized_block.html#a-mixed-effects-model-for-the-randomized-block-design",
    "title": "Beyond MLR Lab 2: Randomized Block Designs",
    "section": "A Mixed Effects Model for the Randomized Block Design",
    "text": "A Mixed Effects Model for the Randomized Block Design\nIn a mixed-effects model, both fixed effects and random effects are used to account for different sources of variation.\nIn the wound healing example, it is essential to account for the laboratory facility because differences between laboratories (such as variations in environmental conditions) could influence the outcome. By including laboratory as a blocking variable, we can more efficiently estimate the treatment effects by controlling for this source of variability. This can be statistically achieved by modeling laboratory as either a fixed effect or a random effect.\nWhile we could model laboratory as a fixed effect, treating it as a random effect is preferred in this case for two reasons: first, it better reflects reality, as the laboratories are considered a random sample of possible laboratory conditions; second, it reduces the number of parameters we need to estimate, making the model more parsimonious. Modeling the lab as random allows us to account for variability between laboratories without estimating a separate effect for each one, which ultimately helps to isolate the treatment effects more effectively.\nOn the other hand, treatment is modeled as a fixed effect because we are specifically interested in estimating and comparing the effectiveness of the three particular wound healing treatments (Treatment A, Treatment B, and Treatment C). These treatments are not considered random samples from a larger population of treatments; rather, they are the specific interventions under study. By modeling treatment as a fixed effect, we aim to draw conclusions about the differences between these particular treatments, and their impact on wound healing.\n\nModel Specification\nThe mixed-effects model for our randomized block design can be specified as:\n\\[\nY_{ij} = \\mu + \\tau_i + b_j + \\epsilon_{ij}\n\\]\nwhere:\n\n\\(Y_{ij}\\): The observed wound healing response for treatment \\(i\\) in laboratory \\(j\\).\n\\(\\mu\\): The overall mean response across all treatments and laboratories.\n\\(\\tau_i\\): The fixed effect of treatment \\(i\\), representing the deviation of treatment \\(i\\) from the overall mean \\(\\mu\\).\n\\(b_j\\): The random effect of laboratory \\(j\\), assumed to be normally distributed with mean zero and variance \\(\\sigma_b^2\\)).\n\\(\\epsilon_{ij}\\): The residual error term, assumed to be independent and normally distributed with mean zero and variance \\(\\sigma^2\\).\n\n\n\n\n\n\n\nNoteNotation convention\n\n\n\nIn mixed-effects models, fixed effects are typically represented by Greek letters, while random effects are represented by Roman letters. This helps differentiate between the two types of effects in model specification.\n\n\n\n\nModel Estimation\nTo fit the previously specified mixed effects model, we use the lmer() function from the lmeTest package:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))  # Effects coding\ncontrasts(data_block$Treatment) # Print contrast matrix for Treatment\n\nmodel_block &lt;- lmer(WoundHealing ~ Treatment + (1 | Laboratory), data = data_block)\nsummary(model_block)\n\n\nSyntax overview\nLet’s break down the model formula WoundHealing ~ Treatment + (1 | Laboratory)\n\nWoundHealing ~ Treatment: This specifies that WoundHealing is the outcome variable and that Treatment is included as a fixed effect to estimate differences between the three treatments. Note that the intercept is included by default in the model, so WoundHealing ~ Treatment is essentially a shortcut for WoundHealing ~ 1 + Treatment. Also remember that with effects coding, the intercept represents the overall mean across all levels of the treatment variable, and the treatment coefficients represent deviations from this mean.\n(1 | Laboratory): This specifies the random effect for Laboratory, representing laboratory-specific deviations from the overall mean.\n\n\n\nSummary overview\nWhen you run summary(model_block), the output will display:\n\nFixed Effects: Estimates of the fixed effects\n\nIntercept: The overall mean wound healing score averaged across all treatments and laboratories.\nTreatment: The differences in wound healing associated with each treatment relative to the overall mean.\n\nRandom Effects: Estimates of the variance components\n\nLaboratory - Intercept: Between-laboratory variability (\\(\\sigma^2_{b}\\)).\nResidual: Residual variance (\\(\\sigma^2\\)).\n\n\n\n\n\n\n\n\nWarningCaution: Interpreting p-values in Mixed Effects Models\n\n\n\nWhile the lmerTest package provides p-values for fixed effects, it is important to use them with caution, primarily because methods for calculating degrees of freedom, such as Satterthwaite, are approximations that may not always be reliable, particularly in complex models fitted to imbalanced data. Hypothesis testing based on likelihood ratio tests may be preferred in those settings.\n\n\n\n\n\nAssessing the fixed effects\nIn addition to providing p-values for fixed effects estimates, the lmerTest package allows us to assess the overall significance of fixed effects using the anova() function. To further explore the nature of these effects, we can obtain the estimated marginal means for the factor levels in the model. Rather than using the package’s build in ls_means() function, we will use emmeans() function from the emmeans package, which provides more options and greater flexibility.\n\n# Obtain ANOVA table for the fixed effects\nanova(model_block)\n\n\n# Compute estimated marginal means for the Treatment variable\nemms &lt;- emmeans(model_block, ~ Treatment)\nemms\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat does the F-test tell us about the treatment effect?\n\n\n\nPairwise comparisons\nSince we found a significant treatment effect, we proceed with computing pairwise differences of the estimated marginal means:\n\n# Perform pairwise comparisons with Bonferroni adjustment\npairs(emms, adjust = \"bonferroni\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the results of the pairwise comparisons, which treatment groups differ significantly?\n\n\n\n\n\nAssessing the variance components\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the estimated variance components from summary(model_block), calculate the intra-class correlation (ICC) and interpret its meaning.\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nConsidering these results, do you think that blocking by laboratory was necessary for this experiment?\n\n\n\n\nModel Diagnostics\nIn mixed-effects models, model diagnostics involve analyzing different types of residuals to evaluate how well the model fits the data and to assess whether key assumptions are satisfied.\nThe default type of residuals used in these models are the conditional residuals, which are calculated based on predicted values that incorporate both fixed effects and estimated random effects. Conditional residuals represent the deviation of the observed data from the model’s predictions, accounting for variability from both sources: fixed effects and random effects. They can be considered estimates of the errors \\(\\epsilon_{ij}\\), which are assumed to be normally distributed with a mean of zero and constant variance.\nTo check these assumptions, we can generate diagnostic plots similar to those used in linear regression models:\n\nResiduals vs. Fitted Plot: This plot helps check for homoscedasticity (constant variance).\nNormal Q-Q Plot: This plot assesses whether the residuals follow a normal distribution.\n\n\n# Extract conditional residuals\nresiduals_cond &lt;- resid(model_block)\n\n# Residuals vs Fitted\nplot(fitted(model_block), residuals_cond,\n     main = \"Residuals vs Fitted\",\n     xlab = \"Fitted values\",\n     ylab = \"Residuals\")\nabline(h = 0, col = \"red\")\n\n# Normal Q-Q Plot\nqqnorm(residuals_cond)\nqqline(residuals_cond, col = \"red\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDo the diagnostic plots suggest any violations of the model assumptions regarding the error term?"
  },
  {
    "objectID": "BLR_lab_two_way_factorial.html",
    "href": "BLR_lab_two_way_factorial.html",
    "title": "Beyond MLR Lab 3: Two-Way Factorial ANOVA",
    "section": "",
    "text": "NotePreliminary setup\n\n\n\nIn this lab, we will use the R packages ggplot2, dplyr, and emmeans. You can use the script below to automatically install and load them using the pacman package.\n\n# Check whether pacman is available and install if needed\noptions(repos = c(CRAN = \"https://cloud.r-project.org\"))\nif (!requireNamespace(\"pacman\", quietly = TRUE)) install.packages(\"pacman\")\n\n# Use pacman to install (if needed) and load the required packages\npacman::p_load(dplyr, ggplot2, emmeans)"
  },
  {
    "objectID": "BLR_lab_two_way_factorial.html#exploratory-data-analysis",
    "href": "BLR_lab_two_way_factorial.html#exploratory-data-analysis",
    "title": "Beyond MLR Lab 3: Two-Way Factorial ANOVA",
    "section": "Exploratory Data Analysis",
    "text": "Exploratory Data Analysis\nTo understand how recovery rates vary across therapy type and disease severity, we start by creating an interaction plot. An interaction plot is a graphical tool that helps visualize whether the effect of one factor (in this case, therapy) depends on the levels of another factor (in this case, disease severity). If the lines on the interaction plot are parallel, this suggests there is no interaction, meaning the effect of therapy is the same for both disease severities. If the lines are not parallel, this indicates a potential interaction, meaning the effect of therapy differs depending on the severity of the disease.\n\n# Visualizing the results\nggplot(data_twoway, aes(x = Therapy, y = RecoveryRate, color = Severity, group = Severity)) +\n  stat_summary(fun = mean, geom = \"point\", size = 3) +\n  stat_summary(fun = mean, geom = \"line\") +\n  labs(title = NULL,\n       x = \"Therapy Type\", \n       y = \"Mean recovery rate (%)\",\n       color = \"Disease Severity\") +\n  theme_minimal()\n\nWe also calculate summary statistics for each combination of therapy type and disease severity:\n\n# Generating summary statistics by Therapy and Severity\nsummary_stats &lt;- data_twoway |&gt;\n  group_by(Therapy, Severity) |&gt;\n  summarise(\n    n = n(),\n    Mean_RecoveryRate = mean(RecoveryRate),\n    SD_RecoveryRate = sd(RecoveryRate),\n    .groups = \"drop\"\n  )\nsummary_stats\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nBased on the plot and summary statistics, does there appear to be an interaction between therapy type and disease severity?"
  },
  {
    "objectID": "BLR_lab_two_way_factorial.html#two-way-factorial-anova-model",
    "href": "BLR_lab_two_way_factorial.html#two-way-factorial-anova-model",
    "title": "Beyond MLR Lab 3: Two-Way Factorial ANOVA",
    "section": "Two-Way Factorial ANOVA Model",
    "text": "Two-Way Factorial ANOVA Model\n\nModel Specification\nUsing effects coding, the two-way factorial Analysis of Variance (ANOVA) model can be specified as:\n\\[\nY_{ijk} = \\mu + \\alpha_i + \\beta_j + (\\alpha\\beta)_{ij} + \\epsilon_{ijk}\n\\]\nwhere:\n\n\\(Y_{ijk}\\): The recovery rate for the \\(k\\)-th patient with therapy type \\(i\\) and disease severity level \\(j\\).\n\\(\\mu\\): The grand mean (overall average recovery rate across all groups).\n\\(\\alpha_i\\): The main effect of therapy type \\(i\\), representing the deviation from the grand mean. Constraint: \\(\\sum_i \\alpha_i = 0\\).\n\\(\\beta_j\\): The main effect of disease severity level \\(j\\), representing the deviation from the grand mean. Constraint: \\(\\sum_j \\beta_j = 0\\).\n\\((\\alpha\\beta)_{ij}\\): The interaction effect between therapy type \\(i\\) and severity level \\(j\\), representing the deviation from additivity. Constraint: \\(\\sum_i (\\alpha\\beta)_{ij} = \\sum_j (\\alpha\\beta)_{ij} = 0\\).\n\\(\\epsilon_{ijk}\\): The residual error term, assumed to be independent and normally distributed with mean zero and constant variance.\n\n\n\nModel Estimation\nWe can fit the two-way ANOVA model using the lm() function. The key feature of the model formula is the interaction term (specified using *), which includes both main effects and their interaction:\n\noptions(contrasts = c(\"contr.sum\", \"contr.poly\"))  # Effects coding\n\n# Fit the two-way ANOVA model with interaction\nmodel_twoway &lt;- lm(RecoveryRate ~ Therapy * Severity, data = data_twoway)\nsummary(model_twoway)\n\n\n\nANOVA Table and Global Hypothesis Tests\nTo test whether the main effects and interaction are statistically significant, we use the anova() function to obtain the ANOVA table:\n\n# ANOVA table\nanova_results &lt;- anova(model_twoway)\nanova_results\n\nThe ANOVA table tests three global hypotheses:\n\nNo interaction: \\(H_0\\!:\\,(\\alpha\\beta)_{ij}=0\\) for all \\(i,j\\)\nNo main effect of Therapy: \\(H_0\\!:\\,\\alpha_i=0\\) for all \\(i\\)\nNo main effect of Severity: \\(H_0\\!:\\,\\beta_j=0\\) for all \\(j\\)\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nWhat do the F-tests and p-values in the ANOVA table tell us about the significance of the interaction and main effects?"
  },
  {
    "objectID": "BLR_lab_two_way_factorial.html#simple-effects-analysis",
    "href": "BLR_lab_two_way_factorial.html#simple-effects-analysis",
    "title": "Beyond MLR Lab 3: Two-Way Factorial ANOVA",
    "section": "Simple Effects Analysis",
    "text": "Simple Effects Analysis\nBecause the interaction effect is significant, we proceed with examining the simple effects: the effect of one factor within each level of the other factor. When analyzing an interaction, we have a choice in how to decompose it. We could examine either the effect of therapy type (Standard vs Enhanced vs Control) separately for each disease severity level, or alternatively, we could examine the effect of disease severity (Mild vs Severe) separately for each therapy type.\nIn this clinical context, we choose to examine the effect of therapy type conditional on disease severity level. This choice is clinically motivated: from a treatment planning perspective, clinicians need to know which therapies work best for patients presenting with different disease severity levels. Understanding how therapy effectiveness varies across severity groups directly informs treatment decisions and allows for severity-stratified recommendations. In other words, the question “which therapy works best for mild cases versus severe cases?” is more clinically actionable than asking “how does severity affect outcomes for each therapy type?”\n\n# Compute simple effects: effect of therapy type within each severity level\nse_severity &lt;- emmeans(model_twoway, ~ Therapy | Severity)\nse_severity\n\nTo test which simple effects are significant, we perform pairwise comparisons within each severity level (using the Bonferroni correction to adjust for multiplicity):\n\n# Pairwise comparisons (Therapy types) within each disease severity level\nse_pairs &lt;- pairs(se_severity, adjust = \"bonferroni\")\nse_pairs\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nFor which disease severity level(s) does therapy type have a significant effect on recovery rate? What is the practical interpretation of this finding?\n\n\n\nAlternative Decomposition: Simple Effects Conditional on Therapy Type\nAs mentioned earlier, we could also decompose the interaction by examining the effect of disease severity separately for each therapy type. While this perspective is less clinically motivated than the severity-stratified approach above, it can still provide useful insights.\n\n\n\n\n\n\nImportantExercise\n\n\n\nPerform a simple effects analysis conditional on therapy type (i.e., examine the effect of disease severity (Mild vs Severe) within each therapy type).\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nFor which therapy type(s) does disease severity have a significant effect on recovery rate?\n\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nHow do these findings complement or differ from the severity-stratified analysis we performed above?\n\n\n\n\nModel Diagnostics\nTo assess the adequacy of the fitted model, we create two diagnostic plots:\n\nResiduals vs Fitted Plot: Used to assess homoscedasticity (constant variance). A random scatter around zero suggests equal variance.\nNormal Q-Q Plot: Used to assess normality of the errors. Points following the reference line suggest normally distributed residuals.\n\n\n# Residuals vs Fitted\nplot(model_twoway, which = 1, main = \"Residuals vs Fitted\")\n\n# Normal Q-Q\nplot(model_twoway, which = 2, main = \"Normal Q-Q\")\n\n\n\n\n\n\n\nImportantQuestion\n\n\n\nDo the diagnostic plots suggest any violations of the model assumptions?"
  },
  {
    "objectID": "Assignments/Week 1/Assignment_week_1.html",
    "href": "Assignments/Week 1/Assignment_week_1.html",
    "title": "Assignment week 1: Effect of Rehabilitation Programs on Mobility in Stroke Patients",
    "section": "",
    "text": "In this assignment, you will analyze a dataset that examines the effect of different rehabilitation programs on the mobility of stroke patients. The dataset includes mobility improvement measurements taken using the 6-Minute Walk Test (6MWT), a standard assessment to evaluate physical mobility and endurance.\nPatients in the study were randomly assigned to one of three rehabilitation programs:\n\nControl: Standard physical therapy.\nRobotic-Assisted Therapy: Rehabilitation involving robotic devices designed to support movement.\nAquatic Therapy: Water-based exercises aimed at improving mobility with reduced joint stress."
  },
  {
    "objectID": "Assignments/Week 1/Assignment_week_1.html#introduction",
    "href": "Assignments/Week 1/Assignment_week_1.html#introduction",
    "title": "Assignment week 1: Effect of Rehabilitation Programs on Mobility in Stroke Patients",
    "section": "",
    "text": "In this assignment, you will analyze a dataset that examines the effect of different rehabilitation programs on the mobility of stroke patients. The dataset includes mobility improvement measurements taken using the 6-Minute Walk Test (6MWT), a standard assessment to evaluate physical mobility and endurance.\nPatients in the study were randomly assigned to one of three rehabilitation programs:\n\nControl: Standard physical therapy.\nRobotic-Assisted Therapy: Rehabilitation involving robotic devices designed to support movement.\nAquatic Therapy: Water-based exercises aimed at improving mobility with reduced joint stress."
  },
  {
    "objectID": "Assignments/Week 1/Assignment_week_1.html#objective",
    "href": "Assignments/Week 1/Assignment_week_1.html#objective",
    "title": "Assignment week 1: Effect of Rehabilitation Programs on Mobility in Stroke Patients",
    "section": "Objective",
    "text": "Objective\nYour objective is to determine whether there is a statistically significant difference in mobility improvement among the three rehabilitation groups. To complete this analysis, follow these steps:\n\nExplore the Data: Perform an exploratory data analysis (EDA) to understand the distribution of mobility improvements across treatment groups.\nFit a One-Way ANOVA Model: Fit a one-way ANOVA model to test for overall group differences.\nPost-Hoc Comparisons: If you identify significant group differences, calculate estimated marginal means and conduct post-hoc comparisons to determine which groups differ.\nModel Diagnostics: Conduct diagnostic tests and visualizations to evaluate model assumptions, including homoscedasticity and normality of residuals."
  },
  {
    "objectID": "Assignments/Week 1/Assignment_week_1.html#requirements-for-the-report",
    "href": "Assignments/Week 1/Assignment_week_1.html#requirements-for-the-report",
    "title": "Assignment week 1: Effect of Rehabilitation Programs on Mobility in Stroke Patients",
    "section": "Requirements for the Report",
    "text": "Requirements for the Report\nSubmit both the Quarto source file (.qmd) and the rendered HTML file. The quarto source file should include all R code and annotations, and the html file should be rendered with code visibility enabled (echo: true, which is the default setting used in the template). The Quarto file in the downloads section below provides a template that you can use for this assignment."
  },
  {
    "objectID": "Assignments/Week 1/Assignment_week_1.html#downloads",
    "href": "Assignments/Week 1/Assignment_week_1.html#downloads",
    "title": "Assignment week 1: Effect of Rehabilitation Programs on Mobility in Stroke Patients",
    "section": "Downloads",
    "text": "Downloads\n\nDownload dataset\nDownload Quarto Template"
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html",
    "href": "Assignments/Week 3/Assignment_week_3.html",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "",
    "text": "In this assignment, you will analyze a dataset collected by the Inner London Education Authority (ILEA) to examine the effectiveness of schools. The dataset consists of examination records of 15,362 students from 140 secondary schools over the years 1985, 1986, and 1987. This dataset was sourced from the data library of the Centre for Multilevel Modelling at the University of Bristol."
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#introduction",
    "href": "Assignments/Week 3/Assignment_week_3.html#introduction",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "",
    "text": "In this assignment, you will analyze a dataset collected by the Inner London Education Authority (ILEA) to examine the effectiveness of schools. The dataset consists of examination records of 15,362 students from 140 secondary schools over the years 1985, 1986, and 1987. This dataset was sourced from the data library of the Centre for Multilevel Modelling at the University of Bristol."
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#dataset-description",
    "href": "Assignments/Week 3/Assignment_week_3.html#dataset-description",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "Dataset Description",
    "text": "Dataset Description\nThe dataset consists of the following variables:\n\nSchool: A numeric variable representing the school identifier (codes 1 to 139)\nExamScore: A numeric variable representing the exam score of each student\nPercentFSM: The percentage of students in the school eligible for free school meals (an indicator of socioeconomic status)\nGender: A categorical variable representing the gender of the student\nVRBand: The verbal reasoning band of the student (VR1, VR2, or VR3). VR band is an indicator of the student’s assessed ability level, with VR1 typically representing the highest ability group, VR2 representing intermediate ability, and VR3 representing lower ability.\nSchoolDenomination: The denomination of the school (Maintained, Church of England, Roman Catholic)"
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#objective",
    "href": "Assignments/Week 3/Assignment_week_3.html#objective",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "Objective",
    "text": "Objective\nYour goals are to determine:\n\nHow much of the variability in exam scores can be attributed to:\n\nDifferences between students\nDifferences between schools\n\nWhether individual-level factors (e.g., Gender, VRBand) significantly influence student exam scores\nWhether school-level factors (e.g., PercentFSM) significantly influence student exam scores"
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#steps-to-complete-the-analysis",
    "href": "Assignments/Week 3/Assignment_week_3.html#steps-to-complete-the-analysis",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "Steps to Complete the Analysis",
    "text": "Steps to Complete the Analysis\n\nLoad the Dataset\nDownload the dataset (see link in the Downloads section below) and read it into R. Make sure all character variables are correctly encoded as factors.\n\n\n\n\n\n\nTipHint\n\n\n\nWhen loading the data, convert the School variable to a factor. This will make it easier to use reorder() in ggplot2 when creating visualizations that order schools by their mean exam scores.\n\n# Load the data\nilea_data &lt;- read.csv(\"downloads/ilea_data.csv\")\n\n# Convert all character variables into factors\nilea_data &lt;- ilea_data %&gt;%\n  mutate(across(where(is.character), as.factor)) %&gt;%\n  mutate(School = as.factor(School))\n\n\n\n\n\nExploratory Data Analysis (EDA):\n\nExplore the distribution of the outcome variable (ExamScore)\nExplore the variance components (i.e., make a plot displaying the within- and between-school variation)\nExplore the relationships between fixed effects and the outcome variable\n\n\n\nMulti-Level Modeling\n\nStart by fitting a null model (i.e., a random intercept model without any predictors) to partition the variance in exam scores between students and schools. Assess how much of the variability in exam scores is due to differences between students and schools.\nNext, add individual-level variables (Gender, VRBand) to the model to explore how these factors influence exam scores.\nAfter that, add context-level variables (e.g., PercentFSM) to your model to understand how factors at the school level contribute to differences in exam scores.\nFinally, explore potential cross-level interactions between individual-level and context-level variables.\n\n\n\nInterpret the Results\nInterpret the findings from your analysis, focusing on the variance components and the effects of individual-level and context-level variables."
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#requirements-for-the-report",
    "href": "Assignments/Week 3/Assignment_week_3.html#requirements-for-the-report",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "Requirements for the Report",
    "text": "Requirements for the Report\nSubmit both the Quarto source file (.qmd) and the rendered HTML file. The HTML file should be rendered with code visibility enabled (echo = TRUE)."
  },
  {
    "objectID": "Assignments/Week 3/Assignment_week_3.html#downloads",
    "href": "Assignments/Week 3/Assignment_week_3.html#downloads",
    "title": "Assignment Week 3: Multilevel Analysis of School Effectiveness",
    "section": "Downloads",
    "text": "Downloads\nDownload dataset"
  }
]