---
title: "Beyond MLR - Week 1: Experimental Design I"
subtitle: "One-way FE/RE, ANOVA, coding, ICC"
author: "Dr. Douwe Postmus"
format:
  revealjs:
    theme: [default, custom-reveal.scss]
    slide-number: true
    toc: false
    slide-level: 2
    incremental: false
    code-overflow: wrap
    scrollable: true
    smaller: true
    width: 1280
    height: 720
    margin: 0.06
    max-scale: 2
    footer: "Beyond linear regression - week 1"
    logo: UMCG-logo.png
execute:
  warning: false
  message: false
  eval: true
  echo: false
---

```{r}
#| include: false
#| message: false
#| warning: false
options(repos = c(CRAN = "https://cloud.r-project.org"))
if (!requireNamespace("pacman", quietly = TRUE)) install.packages("pacman")
pacman::p_load(knitr, rmarkdown, dplyr, ggplot2, emmeans)
```

## Introduction

- Welcome slide (to be completed later)

## Case study — antihypertensive treatments

- Research question: do two experimental drugs (Drug A, Drug B) reduce systolic blood pressure (SBP) more than Control over 12 weeks?
- Design: 120 patients with hypertension are randomly assigned to one of three groups (n = 40 each): Control, Drug A, or Drug B
- Response (outcome): change in SBP (mmHg) from baseline to week 12

```{r}
#| echo: false
# Prepare example data once for subsequent slides (hidden)
set.seed(123)
control <- rnorm(40, mean = -3,  sd = 15)
drugA   <- rnorm(40, mean = -10, sd = 15)
drugB   <- rnorm(40, mean = -5,  sd = 15)
df_bp <- data.frame(
  Treatment = factor(rep(c("Control","DrugA","DrugB"), each = 40),
                     levels = c("Control","DrugA","DrugB")),
  SBP_change = c(control, drugA, drugB)
)
```

## Exploratory data analysis

:::: {.columns}

::: {.column width="50%"}
```{r}
#| echo: false
boxplot(SBP_change ~ Treatment, data = df_bp,
        col = c("gray85","skyblue","lightgreen"),
        ylab = "Change in SBP (mmHg)", main = NULL)
```
:::

::: {.column width="50%"}
```{r}
#| echo: false
aggregate(SBP_change ~ Treatment, data = df_bp,
          FUN = function(x) c(Mean = round(mean(x),2), SD = round(sd(x),2)))
```
:::

::::

## Statistical model specification

To compare the average change in SBP across the three treatment groups, we use the one-way ANOVA model:

$$
Y_{ij} = \mu + \tau_i + \varepsilon_{ij}
$$

- Model components:
  - $Y_{ij}$: change in SBP observed for the $j$-th subject in the $i$-th treatment group
  - $\mu$ (grand mean): the overall mean across groups (the average of the group means)
  - $\tau_i$ (treatment deviations): the deviation of the mean of group $i$ from the grand mean; constrained so $\sum_i \tau_i = 0$
  - $\varepsilon_{ij}$ (residual error): the deviation for subject $j$ from the mean of group $i$
- Statistical assumptions:
  - $\varepsilon_{ij}\sim\mathcal{N}(0,\sigma^2)$: errors are independent and identically normally distributed with mean zero and common variance $\sigma^2$ (homoscedasticity)

## Estimation using linear regression

- A one-way ANOVA model can be expressed as a linear regression with a categorical predictor, allowing us to use the regression framework for estimation and hypothesis testing
- Using dummy coding with the control group as reference, the statistically equivalent regression model is:

:::: {.columns}

::: {.column width="50%"}
$$
Y_{ij} = \beta_0 + \beta_1 D_{1,ij} + \beta_2 D_{2,ij} + \varepsilon_{ij}
$$
:::

::: {.column width="50%"}
| Treatment | Intercept | D1 (DrugA) | D2 (DrugB) |
|---|---:|:---:|:---:|
| Control | 1 | 0 | 0 |
| DrugA   | 1 | 1 | 0 |
| DrugB   | 1 | 0 | 1 |

:::

::::

- Interpretation:
  - Intercept ($\beta_0$): estimated mean for Control
  - $\beta_1$: estimated difference mean(DrugA) − mean(Control)
  - $\beta_2$: estimated difference mean(DrugB) − mean(Control)

## Estimated regression coefficients

```{r}
#| echo: false
# R (refresher): reference coding with Control as baseline
contrasts(df_bp$Treatment) <- contr.treatment(levels(df_bp$Treatment))
fit <- lm(SBP_change ~ Treatment, data = df_bp)
summary(fit)
```

## Mapping to the ANOVA model parameters

- Intercept and betas → group means:
  - mean(Control) = $\beta_0$
  - mean(DrugA)   = $\beta_0 + \beta_1$
  - mean(DrugB)   = $\beta_0 + \beta_2$

- Grand mean (ANOVA intercept):
  $\mu = \frac{\text{mean(Control)} + \text{mean(DrugA)} + \text{mean(DrugB)}}{3} = \beta_0 + \frac{\beta_1 + \beta_2}{3}$

- Treatment deviations (ANOVA $\tau_i$) written in terms of the regression coefficients:
  - $\tau_{\text{Control}} = \text{mean(Control)} - \mu = -\frac{\beta_1 + \beta_2}{3}$
  - $\tau_{\text{DrugA}}   = \text{mean(DrugA)} - \mu = \beta_1 - \frac{\beta_1 + \beta_2}{3}$
  - $\tau_{\text{DrugB}}   = \text{mean(DrugB)} - \mu = \beta_2 - \frac{\beta_1 + \beta_2}{3}$

## Estimated marginal means (EMMs)

- Regression coefficients describe differences relative to a reference group
- Estimated marginal means (EMMs) express the same model in terms of predicted group means
- EMMs provide a direct and interpretable summary of group outcomes implied by the model
- In a one-way ANOVA model, the EMMs equal the observed group means
- In more complex models, EMMs give adjusted or conditional means that make comparisons fairer
  - With covariates (ANCOVA): EMMs adjust group means to a common covariate value, such as the overall mean
  - With interactions: EMMs provide conditional means for specific combinations of factors, allowing group comparisons at chosen settings

```{r}
#| echo: false
# Compute and display estimated marginal means
emms <- emmeans(fit, ~ Treatment)
summary(emms)
```

## F-test and explained variation

- Hypotheses  
  - $H_0$: all group means are equal ($\tau_{Control} = \tau_{DrugA} = \tau_{DrugB} = 0$)  
  - $H_1$: at least one group mean differs ($\exists\, \tau_i \ne 0$)

- The total variation in the response can be decomposed as 
  
  $$SS_{Total} = SS_{Treatment} + SS_{Residual}$$  

  - $SS_{Treatment}$ represents variation explained by differences between treatment means  
  - $SS_{Residual}$ represents unexplained variation within treatments

- The F statistic compares the variance explained by treatment to the residual variance  
  $$
  F = \frac{MS_{Treatment}}{MS_{Residual}} = \frac{SS_{Treatment}/df_{Treatment}}{SS_{Residual}/df_{Residual}}
  $$

- A large F value indicates that the treatment explains a substantial part of the total variation relative to the residual variation

## Global hypothesis testing - results

```{r}
#| echo: false
anova(fit)
```

- The p-value is $\leq 0.05$, so we reject $H_0$ and conclude that the treatment has a significant effect on the change in SBP


## Post-hoc comparisons

- Since we found a significant effect of treatment, we will conduct pairwise comparisons of the estimated marginal means to identify which specific treatment groups differ significantly from one another
- To account for the increased Type I error risk due to multiple comparisons, we apply the Bonferroni correction to adjust the p-values, ensuring that the overall significance level remains controlled

```{r}
# Performing post-hoc analysis with emmeans
emms <- emmeans(fit, ~ Treatment)
contrast(emms, method="pairwise", adjust="Bonferroni")
```

## Model diagnostics

- To assess the adequacy of the fitted model, we create two diagnostic plots:
  - **Normal Q-Q Plot**: Used to assess normality of the errors
  - **Residuals vs Fitted Plot**: Used to assess homoscedasticity (constant variance) for the errors

:::: {.columns}

::: {.column width="50%"}
```{r}
# Normal Q-Q
plot(fit, which = 2, main = "Normal Q-Q")
```
:::

::: {.column width="50%"}
```{r}
# Residuals vs Fitted
plot(fit, which = 1, main = "Residuals vs Fitted")
```
:::

::::

# Part II

## Nuisance variables

- Nuisance variables are factors that influence the response but are not of primary interest  
- Examples include age, baseline blood pressure, comorbidity, or study center  
- If not controlled, they can  
  - increase random variation → more noise, less precision  
  - introduce systematic differences between treatment groups → biased estimates of treatment effects  
- Randomization helps balance nuisance variables across treatments on average  
- Additional control can be achieved through blocking or covariate adjustment to separate their influence from the treatment effect

## Reducing residual variation: blocking

- Design stage
  - Group experimental units that are similar on a known nuisance factor (e.g., study center, sex, baseline severity)  
  - Randomize treatments within each block to ensure balance across levels of the nuisance variable  (this is known as an orthogonal design) 
  
- Analysis stage
  - Include the block term in the statistical model  

  $$Y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$  

  - This adjustment separates block effects from the residual error and allows treatment effects to be estimated with increased precision  
  
## Example: stratified randomization

- Instead of adjusting afterward, we could randomize **within gender strata**  
- Ensures equal numbers of males and females per treatment  
- Prevents imbalance and reduces unexplained variation *by design*  
- Stratified randomization is conceptually similar to **blocking** with two blocks (Male, Female)  

## From stratification to blocking

- Stratified randomization on gender is a simple case of **blocking**  
- In general:  
  - **Blocking** controls for known sources of variation by grouping similar units  
  - The analysis must include the block term:  
    $$Y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}$$  
- Ignoring the block inflates residual variance and reduces power  
- Next lecture: we formalize this as the **randomized block design**,  
  and later treat the block as a **random effect** to estimate the ICC

# Backup slides

## Linear regression with effects coding

- Another common choice is effects (sum-to-zero) coding
- With this coding the regression coefficients map directly to the ANOVA parameters:

$$
Y_{ij} = \beta_0 + \beta_1 E_{1,ij} + \beta_2 E_{2,ij} + \varepsilon_{ij}
$$

where $E_1, E_2$ are the effect indicators for Control and DrugA (see table below).

| Treatment | Intercept | E1 | E2 |
|---|---:|:---:|:---:|
| Control | 1 | 1 | 0 |
| DrugA   | 1 | 0 | 1 |
| DrugB   | 1 | -1 | -1 |

- Interpretation:
  - Intercept ($\beta_0$): grand mean ($\mu$)
  - $\beta_1$ = $\tau_{\text{Control}}$ (deviation of Control from grand mean)
  - $\beta_2$ = $\tau_{\text{DrugA}}$ (deviation of DrugA from grand mean)
  - The remaining group's deviation follows from the sum-to-zero constraint: $\tau_{\text{DrugB}} = -\beta_1 - \beta_2$

## Estimated regression coefficients (effects coded model)

```{r}
#| echo: false
## R: effects (sum-to-zero) coding
contrasts(df_bp$Treatment) <- contr.sum(levels(df_bp$Treatment))
fit2 <- lm(SBP_change ~ Treatment, data = df_bp)
summary(fit2)
```

## Interpreting the residual error

- The residual error represents variation among individuals that remains after accounting for treatment  
- It captures both measurement noise and natural differences between subjects within the same group  
- In the ANOVA F statistic  
  $$
  F = \frac{MS_{Treatment}}{MS_{Residual}},
  $$  
  the denominator $MS_{Residual}$ estimates the variance of the error term $\varepsilon_{ij}$  
- A large residual variance implies that individual outcomes vary widely within treatments, which reduces the ability to detect treatment differences  
- Reducing this unexplained variation, either by design (blocking, stratification) or by analysis (covariate adjustment), increases the precision and power of the test

## Fixed vs random center effects – conceptual view

- In multi-center or blocked studies, center can be modeled as either fixed or random  
- **Fixed effect**: centers are specific and of intrinsic interest  
  $$
  Y_{ijk} = \mu + \tau_i + \beta_j + \varepsilon_{ijk}
  $$
  - One parameter per center (minus constraint)
  - Inference conditional on included centers
  - No assumption about distribution of $\beta_j$
- **Random effect**: centers are a random sample from a larger population  
  $$
  Y_{ijk} = \mu + \tau_i + b_j + \varepsilon_{ijk}, \quad b_j \sim N(0,\sigma_b^2)
  $$
  - One additional variance component ($\sigma_b^2$)
  - Inference averaged over the population of centers
- The choice determines the target of inference:  
  conditional (fixed) vs population-averaged (random)

## Fixed vs random center effects – statistical implications

- In balanced designs with orthogonality between treatment and center  
  → estimated treatment effects are identical under both models  
- In unbalanced designs  
  - Fixed effects give each center equal weight  
  - Random effects weight centers by precision (more efficient)
- Random effects provide partial pooling across centers  
  → smaller standard errors for treatment estimates on average  
- Fixed effects require estimating many parameters  
  → less efficient when many centers or small per-center sample sizes  
- Random effects introduce one additional assumption  
  → center effects follow a normal distribution with variance $\sigma_b^2$  
- Summary  

  | Aspect | Fixed | Random |
  |:--|:--|:--|
  | Inference scope | specific centers | population of centers |
  | Parameters | many ($\beta_j$) | one variance ($\sigma_b^2$) |
  | Weighting | equal across centers | precision-weighted |
  | Efficiency | lower if many small centers | higher via pooling |
  | Interpretation | conditional | marginal (population-averaged) |

